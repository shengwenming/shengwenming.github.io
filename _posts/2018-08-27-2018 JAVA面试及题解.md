---
layout:     post
title:      JAVA面试及题解
subtitle:
date:       2018-08-25
author:     SWM
header-img: img/back.jpg
catalog: true
tags:
    - JAVA
    

---
## 基础篇

### 基本功

####    1、面向对象的特征
    面向对象的三个基本特征是封装、多态和继承。
    
    **封装**就是把事物抽象成类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏。
    
    **继承**可以使用现有类的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。
    继承实现的三种方式为：实现继承、接口继承和可视继承。
    实现继承是指使用基类的属性和方法而无需额外编码的能力
    接口继承是指仅使用属性和方法名称，但是子类必须提供实现的能力
    可视继承是指子（窗体）类使用基（窗体）类的外观和实现代码的能力。
    
####    2、final、finally、finalize 的区别

    final是修饰符，可以修饰类、成员方法和变量。
    
    修饰类：该类不能被继承
    修饰方法：该方法不能被重写
    修饰变量：该变量是常量
    
    
    finally
    
    是异常处理的一部分，常用于释放资源
    一般来说，finally中的方法一定会被执行。在特殊情况下：在执行finally之前，jvm退出了，
    该方法不能被执行
    
    
    finallize
    
    是Object类的一个方法，用于垃圾回收
    
#### 3、int和Integer的区别

    Integer是int的包装类型，int是基本的数据类型
    Integer变量必须实例化之后才能使用而int不用
    Integer是实际对象的引用，指向此new的Integer对象；int直接存储数据值
    Integer的默认值是null；int的默认值是0
    
#### 4、重载和重写的区别
    重载是让类以统一方式处理不同类型数据的一种手段，他们具有相同的名字，但具有不同参数个数\类型
    1、参数个数、类型、顺序至少有一个不同
    2、不能重载只有返回值不同的方法名
    3、存在于父类和子类、同类中
    
    重写：
    1、参数列表必须完全与被重写的方法相同，否则不能称其为重写而是重载
    2、返回的类型必须一直与被重写的方法返回类型相同，否则不能称其为重写而是重载
    3、访问修饰符的限制一定要大于 被重写方法的访问修饰符（public > protected > default > private ）
    4、重写方法一定不能抛出新的检查异常或者比被重写方法申明更加宽泛的检查型异常
    
#### 5、接口和抽象类的区别

    1、接口和抽象类都不能被直接实例化，如果抽象类要实例化，那么抽象类定义的变量必须指向一个子类对象
    这个子类继承了这个抽象类的所有抽象方法，如果接口要实例化，那么接口定义的变量指向一个子类对象，这个
    子类必须实现这个接口的所有方法
    
    2、抽象类要被子类继承，接口要被子类实现
    3、接口里面只能对方法进行声明，抽象类可以对方法进行声明也可以对方法进行实现
    4、抽象类里面的抽象方法必须全部被子类实现，如果子类不能全部实现，那么子类必须也是抽象类，接口里面
    的方法也必须全部被实现类实现，如果子类不能实现那么子类必须是抽象类。
    5、接口里面的方法只能声明，不能具体实现。这个说明接口时设计的结果，抽象类是重构的结果
    6、抽象类里面可以没有抽象方法。
    7、如果一个类里面有抽象方法那么这个类一定是抽象类。
    8、抽象类中的方法都要被实现，所以抽象方法不能使静态的static，也不能是private
    9、接口（类）可以继承接口，甚至可以继承多个接口。但类只能继承一个类。
    10、抽象级别从高到低：接口 > 抽象类 > 实现类。
    11、抽象类主要是用来抽象类别，接口主要是用来抽象方法功能。当你关注事物本质的时候，请用抽象类
    当你关注操作的时候，请用接口
    12、抽象类的功能应该是远多于接口，但是定义抽象类的代价比较高。因为高级语言一个类只能继承一个父类，
    即在设计的时候要考虑到所有的这个类的子类共有的属性和方法；但是接口却可以继承多个接口，因此接口你只
    需要将特定的动作方法抽象到这个接口即可。也就是说接口的设计具有更大可扩展性，而抽象类设计必须十分
    谨慎
    
#### 6、说说反射的用途及实现       
    反射机制就是JAVA在运行时的一项自观能力，通过这种能力可以彻底的了解自身的情况为下一步的动作做准备。
    
    JAVA的反射机制的实现要借助于4个类：class 、Constructor、Field、Method
    
    JAVA反射的作用：
    在JAVA运行时环境中，对于任意一个类，可以知道这个类有哪些属性和方法，对于任意一个对象可以调用他的
    任意一个方法。
    这种动态获取类的信息以及动态调用对象的方法功能来自于java语言的反射机制
    
    Java反射机制主要提供了以下功能：
    在运行时判断任意一个对象所属的类
    在运行时构造任意一个类对象
    在运行时判断任意一个类所具有的成员变量和方法
    在运行时调用任意一个对象的方法
    
    
#### 7、说说自定义注解的场景及实现  
    跟踪代码的依赖性，实现待提配置文件的功能。比较常见的是Spring等框架中的基于注解配置。还可以生成文档
    常见的@See@param@return等。如@override放在方法签名，如果这个并不是覆盖了超类方法，则编译时就能
    检查出 。
    使用@inteface自定义注解时，自动继承了java.lang.annotation.Annotation接口，由编译程序自动完成
    细节，在定义注解时，不能继承其他注解接口 
    
    
#### 8、HTTP 请求的 GET 与 POST 方式的区别  
    最直观的语义上的区别GET用于获取数据，POST用于提交数据
    GET的参数有长度限制（受限于URL长度，具体取决于浏览器和服务器），而POST则没有
    
#### 9、    session 与 cookie 区别
    1、session在服务端，cookie在客户端（浏览器）
    2、session默认被存在服务器的一个文件里
    3、session的运行以来session id 而 session id是存在cookie中的，也就是说，如果浏览器禁用了cookie，
    同时session也会失效（但是可通过其他方式实现，比如在URL中传递session id）
    4、session可以放在文件、数据库和内存中都可以
    5、用户验证场合一般会用session
    
    维持一个会话的核心就是客户端的唯一标识即session id


#### 10、session 分布式处理

       session复制：
       在支持session复制的web服务器上，通过修改web服务器的配置，可实现将session同步到其他web服务器
       达到每个web服务器上都保存一致的session
       优点：代码上不需要修改和支持
       缺点：需要依赖支持的web服务器，一旦更换成不支持的web服务器就不能使用了，在数据量很大的情况下，
       不仅占用网络资源，而且会导致延迟
       
       适用场景：只适用于web服务器比较少且session数据量小的情况
       可用场景：开原方案tomcat-redis-session-manager
       
       session粘滞：
       将用户的每次请求都通过某种方法强制分发到某一个web服务器上，只要这个服务器上存储了对应session数据，
       就可以实现会话跟踪
       
       优点：使用简单，没有额外开销
       缺点：一旦某个web服务器重启或宕机相对应的session数据将会丢失，而且需要依赖负载均衡机制
       
       使用场景：对稳定性要求不是很高的业务场景
       
       session集中管理：
       在单独的服务器或服务器集群上使用缓存技术，如Redis存储session数据，集中管理所有的session，所有的web
       服务器都从这个存储介质中存取对应的session，实现session共享
       优点：可靠性高，减少web服务器的资源开销
       缺点：实现上有些复杂，配置较多
       适用场景：web服务器较多，要求高可用的情况
       可用方案：Spring session 也可以自己实现，主要是重写
       
       基于cookie管理：
       这种方式每次发起请求的时候都需要将session数据存到cookie中传递给服务端
       优点：不需要依赖外部存储，不需要额外配置
       缺点：不安全。易被盗取或篡改；Cookie数量和长度有限制，需要消耗更多网络带宽
       适用场景：数据不重要、不敏感且数据量小
       
####  11、       JDBC 流程

    1、加载Driver类，注册数据库驱动
    2、通过DriverManager，适用URL,用户名，密码建立连接
    3、通过connection，适用sql语句打开statement对象
    4、执行语句将结果返回resultset
    5、对结果进行处理
    6、倒叙释放资源resultset - preparedstatement - connection
    



#### 12、MVC 设计思想
    MVC是软件架构的思想，将一个软件按照模型、视图、控制器进行划分，其中，模型用来封装业务逻辑，视图用来
    表示逻辑，控制器用来协调模型与视图（视图通过控制器来调用模型，模型返回处理结果也要先交给控制器，由
    控制器来选择合适的视图显示处理结果）。
    1、模型：
    业务逻辑包含了业务数据加工与处理以及相应的基础服务
    2、视图:展现模型处理结果，另外，提供相应的操作界面，方便用户使用
    3、控制器：视图发请求给控制器，由控制器来选择相应的模型来处理；模型返回的结果给控制器，由控制器选择
    合适视图
    
    1、使用MVC思想来设计一个软件，最根本的原因是为了实现模型的复用
    2、代码的维护性更好
    3、方便测试
    
    
    使用MVC，会增加代码量，相应的也会增加软件开发的成本，设计难度也会增加
    
    
#### 13、    equals 与 == 的区别

    1、==判断两个变量或者实例是不是指向一个内存空间
    equals是判断两个变脸或者实例所指向的内存空间的值是不是相同
    2、==指向内存地址进行比较，equals是对字符串的内容比较
    3、==指引用是否相同，equals指的是值是否相同
    
### 集合

#### 1、List 和 Set 和 Map 区别
        Collection是最基本的集合接口，一个Collection代表一组Object，即Collection的元素
        （Elements）。一些collection允许相同元素，而另一些则不行
        所有实现Collection接口的类都必须提供两个标准的构造函数：无参的构造函数用于创建空的Collection
        有一个Collection参数的构造函数用于创建一个新的Collection，这个新的Collection与传入的
        Collection有相同的元素。后一个构造函数允许用户复制一个Collection。
        不论Collection的实际类型如何，他都支持一个iterator（）的方法，该方法返回一个迭代子，使用迭代子
        即可逐一访问Collection的每一个元素。
        
        由Collection接口派生的两个接口是List和Set
        
        List接口：
        List是有序的Collection，使用此接口能够精确的控制每个元素插入的位置。用户能够使用索引（元素在
        List中的位置类似于数组下标）来访问List中的元素这类似于Java数组，List允许有相同的元素。
        实现List接口的常用类有LinkedList、ArrayList、Vector、Stack
        
            LinkedList类：实现了List接口，允许Null元素。此外LinkedList提供额外的get、remove、insert
            方法在LinkedList首部或尾部。
            这些操作使LinkedList可被用作堆栈（stack），队列（queue）或双向队列（deque）。
            注意LinkedList没有同步方法。如果多线程同时访问一个List，必须自己实现访问同步，一种解决
            方法是在创建List时构造一个同步的List：
            List list = Collections.synchronizedList(new LinkedList());
            
            ArrayList类：ArrayList实现了可变大小的数组，它允许所有元素，包括null。ArrayList没有同步
            size、isempty、get、set方法运行时间为常数，但是add方法开销为分摊的常数，添加n各元素的时间
            复杂度为O(n)，其他方法运行时间为线性。
            每个ArrayList实例都有一个容量（Capacity）.即用于存储的数组的大小，这个容量可随着不断添加
            新元素而自动增加，但是增长算法并没有定义。当需要插入大量元素的时候，在插入前可以调用
            ensureCapacity方法来增加ArrayList的容量以提高插入效率。ArrayList也是非同步的。
            
            Vector：Vector非常类似ArrayList,但是Vector是同步的，由Vector创建的Iterator，虽然和ArrayList
            创建的Iterator是同一接口，但是，因为vector是同步的，当一个Vector被创建而且正在被使用，
            另一个线程改变了Vector的状态（例如，添加或删除了一些元素），这时调用Iterator的方法时将抛出
            ConcurrentModificationException，因此必须捕获异常。
            
            Stack类：Stack继承自Vector，实现一个后进先出的堆栈。Stack提供5个额外是的Vector可以被当做
            堆栈所使用。基本的push和pop方法，还有peek方法得到栈顶的元素，empty方法测试堆栈是否为空，
            search方法检测一个元素在堆栈中的位置。Stack刚创建后是空栈。
        
        
        Set接口：
        存入set的每个元素必须是唯一的，set不保存重复元素。加入set的元素必须定义equals方法确保对象
        的唯一性。set的元素是无序的。
            HashSet：为快速查找设计的Set，存入HashSet的对象必须定义hashcode。
            TreeSet：保存次序的Set，底层为树结构，使用它可以从set中提取有序的xulie
            LingkedHashSet：具有HashSet的查询速度，且内部使用链表维护元素的顺序（插入的次序）
            于是在使用迭代器遍历Set时，结果会按插入次序显示。
            
        
        
              
              
        Map：     
            Map是一种映射的数据结构，存储key-value的数据，有put、get、containskey、containsvalue等。
            标准的Java类库中包含了几种不同的Map：HashMap，TreeMap，LinkedHashMap、WeakHashMap、
            IdentityHashMap，它们都有同样的基本接口Map，但是行为、效率、排序策略、保存对象的生命周期和判定“
            键“等价的策略不同
            
            
            HashTable：
            HashTable实现Map接口，任何非空（non-null）的对象都可作为key或者value。HashTable是同步的
            （线程安全）
            
            HashMap：
            HashMap实现Map接口，是非同步的，HashMap允许null，即null value和null key。HashSet就是基于
            HashMap，只使用了HashMap的key作为单个元素存储。
            
            HashMap的访问方式是基于Map的最基础的三种方式，HashMap的存储方式为散列表（哈希表），哈希表
            使用数组加链表的组合的方式进行存储。
            put的过程：
            1、插入第一个元素的时候，需要先初始化数组大小
            2、如果key为null会把这个放到table[0]
            3、求key的hash值
            4、获取对应的数组下标
            5、遍历下标处的链表，看是否有重复的key已经存在，如果有，则直接覆盖，put方法返回旧值
            6、不存在重复的key，将此entry添加到链表中
            
            
            数组初始化：
            当第一个元素插入HashMap的时候做一次数组初始化，就是先确定初始的数组大小，并计算数组扩容的
            阈值。
            
            计算位置：
            使用key的hash值与数组长度做与运算
            
            添加节点到链表：
            找到位置后会先对key判重，如果没有重复，就准备将新值放入到链表的表头
            
            数组扩容：
            resize（）方法用于初始化数组或数组扩容，每次扩容后，容量为原来的2倍，并进行数据迁移
            开始遍历原数组，进行数据迁移。
            如果该数组位置上只有单个元素，那就简单了，简单迁移这个元素就可以了
            
            java8的put：
            第一次put的时候回调用resize，类似java7的第一次put也要初始化数组长度，第一次resize和后续
            扩容有些不一样，因为这次是数组从null初始化到默认的16或自定义。
            如果找到具体的数组下标，如果此位置没有值，那么直接初始化一下Node并防止在这个位置可以了
            如果该位置有数据：
            首先判断 该位置的第一个数据和我们要插入的数据key是不是相等，如果是取出这个节点
            如果该节点代表红黑树的节点，调用红黑树的插值方法
            如果插入的位置是一个链表，那么插入的链表的最后面
            如果超出8个，那么会把链表转化为红黑树
            如果在该链表中找到相等的key（==或equals）
            此时break那么e为链表中[与要插入的新值的 key "相等"]的 node
            那么久将新值覆盖旧值，然后返回旧值。
            
            如果HashMap 由于新插入这个值导致size超过了阈值，需要进行扩容
            
            get方法：
            1、计算key的hash值，根据hash值找到对应数组下标 ：hash&(length-1)
            2、判断该数组该位置处，是否就是我们要找的，如果不是，则第三部
            3、判断元素类型是否是TreeNode，如果是，用红黑树的方法获取数据，如果不是走第四部
            4、遍历链表，知道找到相等（==或equals）的key
            
            
            ConcurrentHashMap：
            是J.U.C的重要成员，他是HashMap的一个线程安全的、支持高并发的版本。在默认理想状态
            下，ConcurrentHashMap可以支持16线程执行并发写操作和任意数量线程的读操作。
            ConcurrentHashMap本质上是一个segment数组，而一个segment实例又包含若干个桶，
            每个桶中都包含一条HashEntry对象链接起来的链表。
            总的来说，ConcurrentHashMap的高效并发机制是通过以下三方面来保证的(具体细节见后文阐述)：
            1、通过锁分段技术保证并发环境下写操作
            2、通过HashEntry的不变性、Volatile变量的内存可见性和加锁重读机制保证高效、安全的读操作
            3、通过不加锁和加锁两种方案控制跨段操作的安全性
            
            
### 线程

#### 1、创建线程的方式及实现
    1、继承Thread类创建线程类，通过start方法开启新线程
    使用方式：
    1、继承Thread类，并重写该类的run方法
    2、new一个实例，调用start方法启动该线程
    
    start和run方法的区别：start方法开启了一个新线程，使线程处于就绪状态；run方法只是在当前线程执行实例方法
    直接调用run方法并没有开启新的线程。
    
    2、实现Runnable接口
    使用方式：
    1、实现Runnable接口，并重写该类的run方法
    2、new一个实例，将该实例作为Thread 的Target创建Thread对象
    3、调用Thread的start方法
    
    3、实现Callable接口
    使用方式：
    1、创建Callable接口的实现类，并实现Call方法      
    2、创建Callable实现类的实例，并用FutrueTask类来包装该实例
    3、使用FutrueTask实例作为target创建Thread实例
    4、调用Thread实例的start方法
    
    
#### 2、    sleep() 、join（）、yield（）有什么区别
    1、Thread.sleep(long)和Thread.yield()都是Thread类的静态方法，而join是通过线程对象来调用
    2、wait（）、notify（）、notifyAll（）这三个方法都是Object的方法
    它们都是用于多线程对共享数据的读取，所以必须在synchronized语句块内使用者三个方法，synchronized用于保护
    共享数据阻止其他线程对共享数据的存取。但是这样程序就不灵活了，这三个方法就是用来灵活控制
    wait方法使当前线程暂停执行并释放对象锁标志，让其他线程进入synchronized数据块，当前线程被放入对象池中。
    当调用notify方法后，将从对象移走一个任意的线程并放到锁标志等待池中，只有锁标志等待池中的线程能够获取
    锁标志，如果锁标志等待池中没有线程那么notify将不起作用
    notifyAll则从对象等待池中移走所有等待那个对象的线程并放到锁标志等待池中。
    
    sleep和wait的区别：主要是CPU的运行机制（1、cpu是否继续执行 2、锁是否释放掉）
    cpu为每个线程划分时间片执行，每个时间片都很短，CPU不停的切换不同的线程，它们看似同事执行 的效果
    
    锁如果被占用，那么执行代码片段是同步的，如果锁被释放掉就允许其他线程继续执行此代码块了。
    
    sleep一段时间之后，往往线程会立即执行，可见cpu一直在为线程分配时间片，如果外层有synchronized
    那么锁并没有释放掉，因此sleep CPU继续执行，锁并没有释放掉
    
    wait一般用于锁机制中而sleep不是sleep是线程方法跟锁没有关系，wait、notify、notifyAll是一起使用的
    用于锁机制
    
    肯定是要释放锁的，因为notify并不会立即调起此线程，因此cpu是不会为其分配时间片的，就是wait线程进入等待池
    cpu不分时间片给他，锁释放掉。
    
    1、sleep：Thread类的方法，必须带一个时间参数，会让当前线程休眠进入阻塞状态并释放cpu，提供其他线程运行
    的机会且不考虑优先级，但如果有同步锁则sleep不会释放锁即其他线程无法获取同步锁。
    
    2、yield 让出CPU调度，Thread类的方法，类似sleep只是不由用户指定暂停时间，并且yield只能让同优先级的线程
    有执行机会。yield只是使当前线程重新回到可执行状态，所以执行yield的线程可能在进入可执行状态后马上又被执行
    调用yield方法只是一个建议，告诉调度器我的工作差不多了，可以让别的有相同优先级的线程使用cpu了，没有任何机制
    保证采纳。
    
    3、wait object类的方法（notify、notifyAll也是），必须放在循环体和同步代码块中，执行该方法的会释放
    锁，进入线程等待池中等待被再次唤醒（notify随即唤醒 notifyAll全部唤醒，线程结束自动唤醒）即放入锁池中竞争锁
    
    4、join 一种特殊的wait 当前线程调用另一个线程的join方法当前线程进入阻塞状态直到另一个线程运行结束等待该线程终止，需要捕获异常
    
    线程的5种状态：
    1、新建 （NEW）新建了一个线程对象
    2、可运行（RUNNABLE）：线程对象创建后。其他线程调用此线程的start方法，该状态的线程位于线程池中，等待被线程调度选中，获取cpu使用权
    3、运行（RUNNING）:可运行状态（RUNNABLE）的线程获取cpu时间片（timeslice），执行程序代码
    4、阻塞（BLOCKED）:阻塞状态是指线程因为某种原因放弃了cpu使用权，也让出了timeslice，暂时停止运行。直到线程进入可运行（RUNNABLE
    ）状态，才有机会再次获取cpu timeslice转到运行状态（running）分三种：
    1、等待阻塞：运行状态的线程执行wait方法，jvm会把该线程放入等待队列中（waiting queue）
    2、同步阻塞：运行状态的线程在获取对象的同步锁时，若该同步锁被别的线程占用，jvm会把该线程放入锁池中（lock pool）
    3、其他阻塞：运行的线程执行sleep 或 join 或i/o 请求，jvm会把线程置为阻塞状态。
    当sleep超时、join等待线程终止或超时、或i/o处理完毕线程重新转入可运行状态。
    
    5、  死亡（DEAD）线程run 、main方法执行结束，或者异常退出了run方法，则该线程结束生命周期，死亡线程不可再次复生
    
    
#### 3、说说 CountDownLatch 原理   
    CountDownLatch是一个同步工具，它主要用线程执行之间的写作，CountDownLatch的作用和Thread.join()
    方法类似，让一些线程阻塞到另一些线程完成一系列的动作之后才被唤醒。在直接创建线程的年代
    java5之前，使用Thread.join(),在线程池出现后，因为线程池中的线程不能直接被引用，所以就必须
    在线程池出现后，因为线程池中的线程不能直接被引用，所以就必须使用CountDownLatch了。
    
    CountDownLatch主要有两个方法，当一个或多个线程调用await方法时，这些线程会阻塞，其他线程调用
    countdown方法会将计数器减一。（调用countdown的线程不会阻塞） ，当计数器变为0的时候，因
    await方法阻塞的线程会被唤醒继续执行。
    
    实现原理：计数器的值有构造函数传入，并用它初始化AQS的state值，当线程调用await方法时会检查state的
    值的是否为0，如果是就直接返回（不会阻塞），如果不是，将表示该节点的线程入列，然后将自身阻塞。
    当其他线程调用countdown方法，计数器会减一，然后判断计数器的值是否为0，当他为零时会唤醒队列
    的第一个节点，由于CountDownLatch使用了AQS的共享模式，所以第一个节点被唤醒后又会唤醒第二个节点，
    以此类推，使得所有因await方法阻塞的线程都能被唤醒而继续执行。
    
    一个CountDownLatch对象，只能使用一次不能重复使用
    
#### 4、说说 CyclicBarrier 原理
    CyclicBarrier的字面意思是可循环使用的屏障，他要做的事情是，让一组线程到达一个屏障（也可以叫同步点）
    时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。线程进入屏障
    通过CyclicBarrier的await方法。
    
    CyclicBarrier默认的构造方法时CyclicBarrier（int parties），其参数表示屏障拦截的线程数量，每个
    线程调用await方法告诉屏障我已到达，然后当前线程被阻塞。
    
    CyclicBarrier还提供了一个更高级的构造函数CyclicBarrier（int parties，Runnable barrierAction）
    用于在线程到达屏障时，优先执行barrierAction这个runnable对象，方便处理更复杂的业务场景。
    
    实现原理：CyclicBarrier在内部定义了一个Lock对象，每当一个线程调用CyclicBarrier的await方法时，
    将剩余拦截的线程数减一，然后判断剩余拦截数是否为0，如果不是，进入Lock对象的条件队列等待，如果是
    执行barrierAction对象的runnable方法。
    
    然后将锁的条件队列中的所有线程放入锁等待队列中，这些线程会一次获取锁、释放锁，接着从await方法返回，
    再从CyclicBarrier的await方法返回。
    
    
    
#### 5、说说 Semaphore 原理
    在java的并发包中，Semaphore表示信号量，Semaphore内部主要通过AQS实现线程的管理，Semaphore
    有两个构造函数，参数permits表示许可数，他最后传递给了AQS的state值，线程运行时首先获得许可，
    如果成功，许可数就减一，线程运行，当前线程运行结束释放许可，许可数就加一，如果许可数为0，则
    获取失败，线程位于AQS的等待队列中，他会被其他释放许可的线程唤醒，在创建Semaphore对象的时候
    还可以指定他的公平性，一般常用非公平的信号量，非公平信号量是指在获取许可时先尝试获取许可，而
    不必关心是否已有需要获取许可的线程位于等待队列中，如果获取失败，才会入列。而公平信号量在获取
    许可时首先要检查等待队列中是否已有线程，如果有则入列。
    
#### 6、    说说 Exchanger 原理
    Exchanger（交换者）是一个用于线程间协作的工具类。Exchanger用于线程间的交换数据，他提供一个
    同步点，在这个同步点两个线程可以交换彼此的数据，这两个线程通过exchange方法交换数据，如果第
    一个线程先执行exchange方法，他会一直等待第二个线程也执行exchange，当两个线程到达同步点时，
    这两个线程就可以交换数据，将本线程生产出来的数据传递给对方，因此使用exchanger的重点是成对
    的线程使用exchange方法，当有一对线程达到了同步点，就会进行数据交换，因此该工具的线程对象
    是成对的。
    exchanger类提供了两个方法，exchange（V x）：用于交换，启动交换并等待另一个线程调用exchange
    ，exchange（V x，long timeout，TimeUnit unit）：用于交换，启动交换并等待另一个线程
    调用exchange，并设置最大等待时间，当等待时间超过timeout便停止等待。
    
    
#### 7、说说 CountDownLatch 与 CyclicBarrier 区别
    CountDownLatch：一个或者多个线程等待其他多个线程完成某件事情之后执行
    CyclicBarrier：多个线程互相等待，直到到达同一个同步点，再继续一起执行
    
    对于CountDownLatch来说，重点是一个线程（多个线程）等待，而其他的N个线程在完成某件事情
    之后可以终止也可以等待。而对于CyclicBarrier。重点是多个线程在任意一个线程没有完成，所有
    线程必须等待。
    
    CountDownLatch是计数器，线程完成一个记录一个，只不过计数器不是递增而是递减，而CyclicBarrier
    更像是一个阀门，需要所有线程都到达，阀门才能打开，然后继续执行。
    
#### 8、ThreadLocal 原理分析
    
    每个线程都可以独立修改属于自己的副本而不会互相影响，从而隔离了线程和线程，避免了线程
    访问实例变量发生安全问题。
    1、ThreadLocal只是操作Thread中的ThreadLocalMap对象的集合。
    2、ThreadLocalMap变量属于线程的内部属性，不同的线程拥有完全不同的ThreadLocalMap变量
    3、线程中的ThreadLocalMap变量的值是在ThreadLocal对象进行set或者get操作时创建的。
    4、使用当前线程的ThreadLocalMap的关键在于使用当前的ThreadLocal的实例作为key来
    存储value值。
    5、ThreadLocal模式至少从两个方面完成了数据访问隔离，即纵向隔离（线程与线程之间的
    ThreadLocalMap不同）和横向隔离（不同的ThreadLocal实例之间互相隔离）
    6、一个线程中所有的局部变量其实存储在该线程自己的同一个map属性中
    7、线程死亡时，线程局部变量会自动回收内存。
    8、线程局部变量通过一个entry保存在map中，该entry的key是一个weakrefrence包装的ThreadLocal。
    value为线程局部变量，key到value的映射通过：Thread.threadLocalHashCode&(INITIAL_CAPACITY-1)
    来完成的。
    9、当线程拥有的局部变量超过容量的2/3（没有扩大容量时是10个），会涉及到ThreadLocalMap中的entry回收
    
            
#### 9、讲讲线程池的实现原理
    线程池的相关类：Executor、ExecutorService、AbstractExecutorService、ThreadPoolExecutor
    
    Executor：Executor 接口只有一个 方法，execute，并且需要 传入一个 Runnable 类型的参数。那么它的作用
    自然是 具体的执行参数传入的任务。
    
    
    ExecutorService接口继承了Executor，并且提供了一些其他方法，比如说：
    1、shutdownNow：关闭线程池，返回放入了线程池但是还没开始执行的线程
    2、submit：执行的任务 允许拥有返回值
    3、invokeAll ：运行把任务放进集合中，进行批量的执行，并且能有返回值
    
    这三个方法也可以说是这个接口重点扩展的方法
    
    
    execute 和 submit 区别：
     1、execute 没有返回值，submit有返回值，可以根据任务有无返回值选择对应的方法
     2、submit方便处理异常 。如果任务可能会抛出异常，而且希望外面的调用者能够感知这些异常，
     那么就需要调用submit方法，通过获取Future.get抛出的异常。
     
     
     AbstractExecutorService是一个抽象类，主要完成了submit方法，invokeAll方法的实现，但是其实他的内部
     还是调用了execute方法。
     
     ThreadPoolExecutor 继承了AbstractExecutorService，并且实现了最重要的execute方法，有两个重要的
     成员变量，workers和workqueue，对于workers变量，主要存在了线程对象Worker，Worker实现了Runable接口，
     而对于workQueue变量，主要存放了需要执行的任务，这样其实可以猜到，整个线程池的实现原理应该是workqueue
     中不断的取出需要执行的任务，放在workers中进行处理。
     另外，当线程池中的线程用完之后，多余的任务会等待，利用BlockingQueue的take方法进行处理。
     首先，这里需要先理解两个概念，我们在创建线程池的时候，通常会指定两个变量，一个是maximumPoolSize，另一
     个是corePoolSize。
        对于maximumPoolSize：指的是线程池中最多允许有多少个线程
        对于corePoolSize：指的是线程池中正在运行的线程
        
      在线程池，有这样的设定，我们加入一个任务进行执行
      
        如果现在线程池中正在运行的线程数量大于corePoolSize指定的值而小于maximumPoolSize指定的值，那么就会
        创建一个线程对该任务进行执行，一旦一个线程被创建运行。
        
        如果线程池中的线程数量大于corePoolSize，那么这个任务执行完毕后，该线程会被回收。如果小于corePoolSize
        那么该线程即使空闲，也不会被回收，下个任务过来，那么就使用这个空闲线程。
        
         大致原理如下：
         首先，各自存放线程和任务，其中任务带有阻塞，
         然后再execute方法中，进行addWorker（command，true），也就是创建一个线程把任务放进去执行或者是
         直接把任务放入到任务队列中。
         
         接着如果是addWorker，那么就会new Worker（task）- 调用其中run（）方法，在worker的run方法中调用
         runWorker（this） - 在该方法中就会具体执行我们的任务task.run()，同时这个runworker方法相当于是
         个死循环，正常情况下就会一直取出任务队列中的任务执行，这保证了线程不会销毁。
         
         这也就是为什么线程池可以避免频繁创建线程和销毁带来的性能消耗。
         
####  10、       线程池的几种方式
    1、newCachedThreadPool：创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收线程，若无可回收
    则新建线程。
    特点是：1、工作线程的创建数量几乎没有限制（其实也有限制的，数目为Integer.MAX_VALUE）,这样可灵活的往
    线程池中添加线程。
    2、如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定时间（默认为1分钟），则该线程会自动终止，终止后
    如果又提交了新的任务，则线程池重新创建一个工作线程。
    3、在使用CachedThreadPool时，要控制任务的数量，否则，由于大量线程同时运行，很可能会造成系统瘫痪
    
    
    2、newFixedThreadPool：创建一个固定长度的线程池，可控制线程的最大并发数，超出的线程在队列中等待。
    
    3、newScheduledThreadPool：创建一个固定长度的线程池，支持定时周期性任务执行。
    
    4、newSingleThreadExecutor：创建一个单线程化的线程池，它只会用唯一的线程来执行任务，保证所有的任务按照指定顺序
    （FIFO,LIFO,优先级）来执行。
    
####  11、   线程的生命周期
     1、新建状态（NEW）
     2、可运行状态（RUNNABLE）
     3、运行状态（RUNNING）
     4、阻塞状态(BLOCKED)
     5、死亡（DEAD）
     
### 锁机制

#### 1、说说线程安全问题     
    当多个线程访问某个方法时，不管你通过怎样的调用方式或者说这些线程如何交替执行，我们的主程序中不需要去做任何的同步
    这个类的结果行为都是我们设想的正确行为，那么我们就可以说这个类是线程安全的。
    
    
#### 2、    volatile 实现原理
    Java内存模型：
    在并发编程领域中一般都会遇到三个概念：原子性、可见性、有序性。
    原子性：一个操作或者多个操作要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。
    
    在单线程环境下可以认为所有的操作都是原子操作，但多线程环境不同，Java只保证基本数据类型的变量和赋值才是原子性的
    （注：在32位JDK环境下，对64位数据的读取不是原子性操作，如long、double）。要想在多线程环境保证原子性则可以通过
    锁、synchronized来确保。
    
    volatile是无法保证复合操作的原子性的。
    
    可见性：
    
    可见性是指当多个线程访问同一变量时，一个线程改变了这个变量的值其他线程可以立即看到。
    
    Java体统了Volatile来保证可见性。
    
    当一个变量被volatile修饰后，表示着线程本地内存无效，当一个线程修改共享变量后，他会被立即刷新到主内存中，当其
    他线程读取变量时，他会直接从内存读取。
    
    当然synchronized和锁都可以保证可见性。
    
    有序性：
    
    即程序执行的顺序按照代码的先后顺序执行。
    
    在Java内存模型中，为了效率是允许编译器和处理器对指令进行重排序，当然重排序他不会影响单线程的运行结果，但是对
    多线程会有影响。
    
    Java提供volatile来保证一定的有序性，
    
    Volatile原理
    
    volatile可以保证县城可见性且提供了一定的有序性，但是无法保证原子性，在JVM底层volatile是采用“内存屏障”实现
    
    1、保证内存可见性，不保证原子性。
    2、禁止指令重排序。
    
    在执行程序时为了提高性能，编译器和处理器常对指令做重排序
    1、编译器重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。
    2、处理器重排序。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。
    
    指令重排序对单线程没有什么影响，他不会影响程序的运行结果，但是会影响多线程的正确性，既然指令重排序会影响到
    多线程执行的正确性，那么就要禁止重排序，
    
    happens-before原则保证了程序的有序性，他规定如果两个操作的执行顺序无法从happens-before原则中推导出来
    那么就不能保证有序性，可以随意进行重排序。
    
    1、同一个线程中，前面的操作happens-before后续的操作（单线程内代码按顺序执行，但是在不影响在单线程环境
    执行结果的前提，编译器和处理器可以进行重排序，这是合法的，换一句话说，这一规则是无法保证编译重排和指令
    重排）    
    2、监视器上的解锁操作happens-before其后续的加锁操作。（synchronized规则）
    3、对volatile变量的写操作happens-before后续的读操作（volatile规则）
    4、线程的start（）方法happens-before该线程所有的后续操作（线程启动规则）
    5、线程的所有操作happens-before其他线程在该线程上调用join返回成功后的操作
    6、如果a happens-before b， b happens-before c ，那么a happens-before c（传递性）
    
    观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出
    一个lock前缀指令，lock前缀指令其实相当于一个内存屏障。内存屏障是一组处理指令，用来实现对内存操作的顺序
    限制，volatile底层就是通过内存屏障来实现的。
    
    volatile相对于synchronized稍微轻量些，在某些场合可以替代synchronized，但是又不能完全取代，只有在某些场
    合才能使用volatile，必须满足以下两个条件：
    1、对变量的写操作不依赖当前值
    2、改变量没有包含在具体其他变量的不变式中
    
    volatile经常用于两个场景状态标记量、double check
    
#### 3、     synchronize 实现原理
    Java虚拟机中的同步（Synchronization）基于进入和退出管程（Monitor）对象实现，无论是显示同步（有明确的
    Monitorenter）和monitorexit指令，即同步代码块）还是隐式同步都是如此。在java语言中，同步用的最多的地方
    可能是被synchronized修饰的同步方法，同步方法并不是由monitorenter和monitorexit指令实现同步的，而是由
    方法调用指令读取运行时常量池中的方法ACC_SYNCHRONIZED标志来隐式实现，关于这点，稍后详细分析。下面先来
    了解一个概念Java对象头，这对深入理解synchronized实现原理非常关键。
    
    理解java对象头与monitor
    
    在JVM中，对象在内存中的布局分为三块区域：对象头、实例数据和对齐填充。
    
    实例变量：存放类的属性信息，包括父类的属性信息，如果是数组的实例部分好包括数组长度，这部分内存按4字节对齐。
    
    填充数据：由于虚拟机要求对象起始地址必须是8字节的整数倍。填充数据不是必须存在的，仅仅是为了字节对齐，这点
    了解即可。
    
    而对于顶部，则是java头对象，他实现synchronized的锁对象的基础，一般而言synchronized使用的锁对象是存储在
    java对象头里的，jvm采用2个字节来存储对象头（如果对象是数组则会分配3个字节，多出来一个字节用于记录长度），
    
    MarkWord在默认情况下存储hashcode、分代年龄、锁标记位等，由于对象头信息与对象自身定义的数据没有关系的额外
    存储成本，因此考虑到JVM的空间效率，MarkWord被设计成为一个非固定的数据结构，以方便存储更多有效的数据，他会
    根据对象本身的状态复用自己的存储空间。
    
    轻量级锁和偏向锁是java 6对synchronized锁进行优化后新增的
    
    重量级锁也就是通常所说的synchronized的对象锁锁标识位为10，其中指针指向的是monitor对象（也称为管理和监
    视器锁）的起始地址，每个对象都存在一个monitor与之关联，对象与其monitor之间的关系存在多种实现方式，如
    monitor可以与对象一起创建销毁或当前线程试图获取对象锁时自动生成，但当一个monitor被某个线程持有后，他便
    处于锁定状态，在java虚拟机中，monitor是由objectmonitor实现的。Monitor对象存在于每个java对象的对象头中
    （存储指针的指向），synchronized锁便是通过这种方式获取锁的 也就是为什么Java中任意对象可以作为锁的原因
    ，同时也是notify、notifyAll、wait等方法存在于顶级对象Object中的原因。
    
    synchronized代码块底层原理：
    
    同步语句块的实现使用的是monitorenter和monitorexit指令，其中monitorenter指令指向指向同步代码块的开始位置
    monitorexit指令则指明同步代码块的结束位置，当执行monitorenter指令时当前线程试图获取objectref（即对象锁）
    所对应的monitor的持有权，当objectref的monitor的进入计数器为0，那么线程成功取得monitor，并将计数器设置为1，
    取锁成功。如果当线程已经拥有objectref的monitor的持有权，那么他可以重入这个monitor，重入时计数器也会加1，
    倘若其他线程已经拥有objectref的monitor的所有权，那么当前线程将被阻塞，直到正在执行线程执行完毕，即
    monitorexit指令被执行，执行线程释放monitor并设置计数器为0，其他线程将有机会持有monitor。值得注意的是
    编译器将会确保无论方法通过何种方式完成，方法中调用过的每条monitorenter指令都有执行其对应的monitorexit，
    无论这个方法是正常结束还是异常结束，为了保证方法异常完成时monitorenter和monitorexit指令依然可以正确配对
    执行，编译器会自动产生一个异常处理器，这个异常处理器声明可处理的所有异常，他的目的就是用来执行monitorexit
    指令，从字节码中也可以看出多了一个monitorexit指令，他就是异常结束时被执行的释放monitor的指令。
    
    synchronized方法底层原理：
    方法级的同步是隐士，即无需通过字节码指令来控制的，他实现在方法调用和返回操作之中 。JVM可以从方法常量池中的
    方法表结构中的 ACC_SYNCHRONIZED访问标志区分一个方法是否同步方法。当方法调用时，调用指令将会检查方法的
    ACC_SYNCHRONIZED访问标志是否被设置，如果设置了，执行线程将先持有monitor(虚拟机规范中用的是管程一词)，然后
    再执行方法，最后再方法完成（无论是正常完成还是非正常完成）时释放monitor在执行期间执行线程持有了monitor，
    其他任何线程都无法再获取同一个monitor，如果一个同步方法执行期间抛出了异常，并且在方法内部无法处理异常，
    那这个同步方法所持有的monitor将在异常抛到同步方法外时自动释放。
    
    
    java虚拟机对synchronized的优化
    
    锁的状态总共有四种，无锁状态、偏向锁、轻量级锁、重量级锁。随着锁的竞争锁可以从偏向锁升级到轻量级锁再升级到
    重量级锁，但是锁的升级是单向的，也就是说只能从低到高，不会出现锁降级
    
    偏向锁：
    偏向锁是java 6 之后加入的新锁，他是一种加锁操作的优化手段，经过研究发现，在大多数情况下，锁不仅不存在多线程
    竞争，而且总是由同一个线程多次获得，因此，为了减少同一个线程获取锁（会涉及到CAS操作，耗时）的代价而引入偏向
    锁，偏向锁的核心思想是，如果一个线程获得了锁，那么锁就进入偏向模式，此时markword的结构变为偏向锁结构，
    当这个线程再次请求锁时，无需任何同步操作，即获取锁的过程。这样就省去了大量有关锁申请的操作，从而也就是提高了
    程序的性能，所以对于锁竞争激烈的场合，偏向锁就失效了，因为这样的场合极有可能每次申请锁的线程都是不同的，因此
    不应该使用偏向锁，得不偿失，另外注意，偏向锁失败后，并不会立即膨胀为重量级锁，而是先升级为轻量级锁。
    
    轻量级锁
    倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，他会尝试使用一种称为轻量级锁的优化手段（1.6之后加入），此时
    markword结构也变为轻量级锁的结构。轻量级锁能够提升程序性能的依据是“对绝大部分的锁，在整个同步期间内不存在竞争”
    这是经验数据，需要了解的是，轻量级锁所适应的场景是线程交替执行同步块的场合，如果存在同一时间访问同一锁的场合
    就会导致轻量级锁膨胀为重量级锁。
    
    自旋锁
    
    轻量级锁失败后，虚拟机为了避免线程真实的的在操作系统层面挂起，还会进行一项称为自旋锁的优化手段，这是基于在大
    多数情况下，线程持有锁的时间都不太长，如果直接挂起操作系统层面的线程可能会得不偿失，毕竟操作系统实现线程之间的
    切换时需要从用户态转换核心态，这个状态之间的转换需要相对比较长的时间，时间成本较高，因此自旋锁会假设在不久将来
    当前线程可以获得锁，因此虚拟机会让当前想要获取锁的线程做几个空循环（这也是成为自旋的原因），一般不会太久，可能
    50到100个循环，在经过若干次循环后，如果得到锁，就顺利进入临界区，如果还不能获得锁，那就会将线程在操作系统层面
    挂起。这是自旋锁的优化方式，这种方式可以提高效率，最后没办法只能升级为重量级锁。
    
    锁消除
    
    锁消除是虚拟机另外一种优化方式，这种优化更彻底，java虚拟机在jit编译时（可以简单理解为当某段代码即将被第一次执
    行时进行编译，又称即时编译），通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过这种方式消除没有
    必要的锁，可以节省毫无意义的请求锁时间。
    
    
#### 4、    synchronized与Lock的区别
    1、sychronized是JAVA的关键字，在JVM层面上，Lock是一个接口类。
    2、以获取锁的线程执行完同步代码，释放锁，线程执行异常，jvm会让线程释放锁，Lock在finally中必须释放锁，不然容易造成
    线程死锁。
    3、假设A线程获得锁，B线程等待，如果A线程阻塞，B线程会一直等待。Lock分情况而定，Lock有多个获取锁的方式，可以尝试获
    得锁，线程不需要一直等待。
    4、锁的状态：synchronized无法判断，Lock可以判断
    5、锁类型：synchronized可重入，不可中断，非公平，Lock可重入，可判断，可公平
    6、性能：synchronized少量同步，Lock大量同步
    

#### 5、CAS 乐观锁    
    Java里面进行多线程通信的主要方式是共享内存的方式，共享内存主要关注的点有两点：可见性和有序性。加上复合操作的原子
    性，我们可认为java线程安全问题主要关注三点：可见性、有序性和原子性。
    
    Java内存模型解决了可见性和有序性问题，而锁解决了原子性问题。
    
    锁存在的问题：
    java在jdk1.5之前都是靠synchronized关键字保证同步的这种通过使用一致的锁定协议来协调对共享状态的访问，可以确保无论
    哪个线程持有共享变量的锁，都采用锁独占的方式访问这些变量，独占锁其实就是一种悲观锁。所以可以说synchronized是悲观
    锁。
    
    悲观锁机制存在以下问题：
    1、在多线程竞争机制下。加锁、释放锁会导致比较多的上下文切换和调度延时引起性能问题。
    
    2、一个线程持有锁会导致其他  所有需要此锁的线程挂起。
    
    3、如果一个优先级高的线程等待一个优先级低的线程释放锁会导致优先级倒置引起性能风险。
    
    而另一个更加有效地锁就是乐观锁，所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作 ，如果因为冲突失败就重试
    ，知道成功为止。
    
    与锁相比volatile变量是一个更轻量级的同步机制，因为在使用这些变量时不会发生上下文切换和线程调度操作，但是volatile不能
    解决原子性问题，因此当一个变量依赖旧值时就不能使用volatile变量。因此对于同步最终还是要回到锁机制上来。
    
    
    乐观锁其实是一种思想，但对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会
    正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误信息，让用户决定如何去做。
    
    主要两个步骤：
    1、冲突检测
    2、数据更新
    
    其实现方式有一种比较典型的就是CAS（Compare And Sawap）
    
    CAS:
    CAS是项乐观锁技术，当多个线程尝试使用CAS同时更新同一个变量时，只是其中一个线程能更新变量的值，而其他线程都失败，失败的
    线程并不会被挂起，而是被告知这次竞争中失败,并可以再次尝试。
    
    CAS操作包含三个操作数--内存位置（V）、预期原值（A）和新值（B）。如果内存位置的值与预期的原值想匹配，那么处理器会自动将
    该位置值更新为新值，否则，处理器不做任何操作，无论哪种情况，他都会在CAS指令之前返回该位置的值。（在CAS的一些特殊情况下
    ，将仅返回CAS是否成功而不提取当前值）CAS有效地说明了“我认为位置V应该包含值A；如果包含该值，则将B放到这个位置；否则不要
    更改该位置，只告诉我现在位置的值即可。”这其实和乐观锁的冲突检查和数据更新的原理是一样的。
    
    这里再强调一下，乐观锁是一种思想，CAS是这种思想的一种实现方式。
    
    Java对CAS的支持：
    在Jdk 1.5中新增J.U.C就是建立在CAS之上的，相对于synchronized这种阻塞算法，CAS是非阻塞算法的一种实现，所以J.U.C在性能上
    有了很大的提升。
    
    ABA问题：
    CAS算法实现一个重要的前提是取出内存中某时刻的数据，而在下时刻比较并替换，那么在这个时间差类会导致数据的变化，比如说线程1 
    从内存位置V中取出A，这时候另一个线程2也从内存中取出A，并且线程2进行了一些操作变成了B，然后线程2又将位置V的数据变成A，这时
    候线程1进行CAS操作发现内存中仍是A，尽管这个线程操作成功，但不是代表这个过程是没问题的 。
    
    部分乐观锁通过版本号（version）的方式解决ABA的问题，乐观锁每次在执行数据的修改操作时，都会带上一个版本号，一旦版本号和
    数据的版本号一致就可以执行修改操作，并对版本号执行+1操作。否则就执行失败。因为每次操作的版本号都会随之增加，所以不会出现
    ABA问题，因为版本号只会增加不会减少。
    
    Java中的线程安全问题至关重要，要想保证线程安全，就需要锁机制，锁机制包含两种乐观锁和悲观锁，悲观锁是独占锁，阻塞锁，乐观锁
    是非独占锁，非阻塞锁，有一种乐观锁的实现方式就是CAS,这种算法在JDK1.5中引入的J.U.C中有广泛的应用，但是值得注意的是这种算法会
    存在ABA问题。
    
    另外CAS还有一个应用，那就是在JVM创建对象的过程中。对象创建在虚拟机中是非常频繁的，即使是仅仅修改一个指针所指向的位置，在并
    发情况下也不是线程安全的，可能正在给对象A分配内存空间，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况，解
    决这个问题的方案有两种，其中一种就是采用CAS配上失败重试的方式保证更新操作的原子性。
    
#### 6、    乐观锁的业务场景及实现方式
    悲观锁：比较适合写入操作比较频繁的场景，如果出现大量的读取操作。每次读取的时候都会加锁，这样会增加大量的锁的开销，降低了系
    统的吞吐量。
    
    乐观锁：比较适合读取操作比较频繁的场景，如果出现大量的写入操作，数据发生冲突的可能性就会加大，为了保证保证数据的一致性，应
    用层需要不断的重新获取数据这样会加大查询操作，降低了系统的吞吐量。
    
    读取频繁使用乐观锁，写入频繁使用悲观锁。
    
    JAVA 1.5 J.U.C的CAS操作实现了乐观锁的思想。为解决ABA问题可以加加入版本号。
    
    
## 核心篇

### 数据存储

#### 1、MySQL 索引使用的注意事项
    1、索引的类型
    普通索引、唯一索引、主键索引、外键索引和组合索引，它们创建索引位置上都是一样的，即在表的一个或多个字段上创建使用。
    
    索引优化：
    1、在经常查询的表字段建立索引
    
    具体创建什么索引，可根据需求来定，如果没有特殊要求，如：是否允许重复，那么就可以创建一普通索引，否则可以创建一个唯一索引，
    如果需要多个索引唯一，那么就创建唯一组合索引即可。
    
    2、在大数据量检索中，尽量使用FULL-TEXT代替LIKE
    使用InnoDB引擎的要升级到mysql5.6
    
    3、维护优化索引碎片
    在建有数据索引的数据表中，每当删除记录数据时，对应记录上的索引标记并未删除，这会产生数据垃圾，也叫碎片，长期以往不做处理的话
    会影像数据的检索效率。
    
    比较好的办法重建索引
    
    4、避免使用聚合函数
    在建有索引的数据表中，尽量在检索条件后不使用聚合函数，这可能会使索引失效，影响数据检索速度。
        
             
#### 2、说说分库与分表设计
    应用场景：使用mysql数据库做查询，当数据量超过200W时，查询数据受到限制，此时为了避开这一个瓶颈，我们采取分库分表的数据库
    设计思想，将数据按照一定规律保存至数据库，常用方式如下：
    1、使用时间作为依据分库分表
    例如可以将数据按月份保存在表中，例如：translate_2015_01,translate_2015_02.......以此达到分表、分库的目的；
    2、使用数字作为分库分表的标准（取余）
    我们经常会定义某个字段自增长或者某个随机数或者特定编码，利用这个编码，我们将其存放在不同的表中，例如，我们想要定义30张表
    存我们获得的数据，第一个值为1，我们用1%30=1，由此可知该数据放在第一张表中，有例如500%30=20，该数据放入第20张表。
    3、使用MD5区分
    在上传文件过程中，将文件名的MD5值取前3位作为上传文件保存的文件夹名称，这样便将上传的文件保存在了3^3文件夹中方便下次找到该
    文件。
    
#### 3、    分库与分表带来的分布式困境与应对之策
    1、数据迁移与扩容问题
    水平分表策略可以分为随机分表和连续分表两种情况，连续分表有可能存在数据热点的问题，有些表可能会被频繁地查询从而造成较大
    压力，热数据的表就成为了整个数据库的瓶颈，而有些表可能存的是历史数据，很少需要被查询到，连续分表的另外一个好处在于比较
    容易，不需要考虑迁移旧的数据，需要添加分表就可以自动扩容，随机分表的数据相对比较均匀，不容易出现热点和并发访问的瓶颈，
    但是分表扩展需要迁移旧的数据。
    针对于水平分表的设计至关重要，需要评估中短期业务的增长速度，对当前的数据量进行容量规划，综合成本因素，推算出大概需要
    多少分片，对于数据迁移的问题，一般做法是通过程序先读出数据，然后按照指定的分表策略再将数据写入到各个分表中。
    
    2、表关联问题
    在单库单表的情况下，联合查询是非常容易的，但是，随着分库与分表的演变，联合查询就遇到跨库关联和跨表关系问题，这设计
    之初就应该尽量避免联合查询，可以通过程序中进行拼装或者通过反范式设计进行规避。
    
    3、分页与排序问题
    一般情况下，列表分页时需要按照指定的字段进行排序，在单库单表的情况下，分页和排序也是非常容易的，但是随着分库与分表的
    演变，也会遇到跨库排序和跨表排序问题，为了   最终结果的准确性，需要在不同的分表中将数据进行排序并返回，并将不同分表
    返回的结果进行汇总和再次排序，最后再返回给用户。
    
    4、分布式事务问题
    随着分库与分表的演变，一定会遇到分布式事务问题，那么如何保证数据的一致性就成为一个必须面对的问题，目前，分布式事务并
    没有很好的解决方案，难以满足数据强一致性，一般情况下，使存储数据尽可能达到用户一致，保证系统经过一段较短的时间的自我
    恢复和修正，数据最终达到一致。
    
    5、分布式全局唯一ID
    在单库单表情况下，直接使用数据库自增特性来生成主键ID，这样确实比较简单，在分库分表 的环境中，数据分布在不同的表中，
    不能再借助数据库自增长特性，需要使用全局唯一ID，例如UUID、GUID等。
    
    分库分表主要应用于应对海量数据和高并发，然而，分库与分表是一把双刃剑，虽然很好的应对海量数据和高并发对数据库的冲击和
    压力，但是却提高的系统的复杂度和维护成本。需要结合实际需求，不宜过度设计，在项目一开始不采用分库与分表设计，而是随着
    业务的增长，在无法继续优化的情况下，再考虑分库与分表提高系统的性能。
    
    
#### 4、    说说 SQL 优化之道
    1、负向条件查询都不能用索引
    
    select * from order where status!=0 and stauts!=1
    
    not in/not exists都不是好习惯
    
    可以优化为in查询：
    
    select * from order where status in(2,3)
    
    
    2、前导模糊查询不能使用索引
    select * from order where desc like '%XX'
    
    而非前导模糊查询则可以：
    
    select * from order where desc like 'XX%'
    
    3、数据区分度不大的字段不宜使用索引
    select * from user where sex=1
    
    原因：性别只有男，女，每次过滤掉的数据很少，不宜使用索引。
    
    经验上，能过滤80%数据时就可以使用索引，对于订单状态，如果状态值很少，不宜使用索引，如果状态值很多，能够过滤大量数据，
    则应该建立索引。
    
    4、在属性上进行计算不能命中索引
    select * from order where YEAR(date) < = '2017'
    
    即使date上建立了索引，也会全表扫描，可优化为值计算：
    select * from order where date < = CURDATE()
    
    select * from order where date < = '2017-01-01'
    
    5、如果业务大部分使用单条查询，使用Hash索引性能更好，例如用户中心
        
    select * from user where uid=?
    
    select * from user where login_name=?
    
    原因：B-TREE   索引的时间复杂度是O(log（n）)；Hash索引的时间复杂度是O（1）    
    
    6、允许null的列，查询有潜在大坑
    
    单列索引不存null值，复合索引不存全为null值，如果列允许null可能会得到不符合预期的结果集
    select * from user where name != 'shenjian'
    
    如果那么允许为null，索引不存储null值，结果集中不会包含这些记录
    
    所以，请使用not null约束以及默认值
    
    7、复合索引最左前缀，并不是指SQL语句的where顺序要和索引一致
    用户中心建立(login_name, passwd)的复合索引
    select * from user where login_name=? and passwd=?
    
    select * from user where passwd=? and login_name=?
    
    都能够命中索引
    
    select * from user where login_name=?
    
    
    也能命中索引，满足复合索引最左前缀
    

    select * from user where passwd=?
    
    不能命中索引，不满足复合索引最左前缀
    
    8、使用ENUM而不是字符串
    
    ENUM保存的是TINYINT，别在枚举中搞一些“中国”、“北京”这样的字符串，字符串空间又大，效率又低。
    
    9、如果明确知道只有一条返回，limit 1能够提高效率
    
    select * from user where login_name=?
    
    可以优化为：
    
    select * from user where login_name=? limit 1
    
    原因：你知道只有一条结果，但数据库并不知道，明确告诉它，让它主动停止游标移动
    
    10、把计算放到业务层而不是数据库层，除了节省数据的CPU还有意想不到的查询缓存优化效果
    
    select * from order where date < = CURDATE()
    
    $curDate = date('Y-m-d');
    
    $res = mysql_query(
    
        'select * from order where date < = $curDate');
        
     原因：
     
     释放了数据库的CPU
     
     多次调用，传入的SQL相同，才可以利用查询缓存
     
     
     11、强制类型转换会全表扫描
     
     select * from user where phone=13800001234
     
     12、尽量使用数字字段
     
     13、尽可能的使用 varchar/nvarchar 代替 char/nchar
     
     14、任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段
     
     15、尽量使用表变量代替临时表，如果表变量包含大量数据，请注意索引非常有限（只有主键索引）
     
     16、避免频繁创建和删除临时表
     
     17、临时表并不是不可使用，适当使用他们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时，但
     是对于一次性事件，最好使用导出表。

    
#### 5、    MySQL 遇到的死锁问题
    死锁是指两个或者两个以上的线程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，他们都将无法推进下去，此时称系统
    出于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程，表级锁不会产生死锁，所以解决死锁主要还是针对于最常用的InnoDB
    
    死锁的关键在于：两个或两个以上的Session加锁的顺序不一致。
    
    那么对应的解决死锁问题的关键是：让不同的Session加锁有次序。
    
    
    设置锁超时时间set lock_timeout：尝试获取锁的时候加一个锁超时时间，如果超过这个时间放弃对该锁请求。比如设置A的超时时间为10ms，B
    为100ms，A试了10ms以后获取不到资源，然后会自动断开，A断开了，这时B就可以获取资源了，避免了死锁。（但这个方法不太好的地方在于，还
    需要对A再次提交，而且timeout的时间需要综合很多其他因素去设置）
    
    对所使用的数据全部加锁：每一个事务一次就将所有要用到的数据全部加锁，否则就不允许执行，比如A在B转钱的时候，会使用到A账户转账前，A转
    账后，B账户转账前，B账户转账后，所以就算是A给B转账，也要把A 、B账户所有信息都一起加锁（这样B想给A转账也不行因为被锁住了，不过这个
    效率很低，可能也会带来其他死锁问题）
    
    设置锁优先级：提前设置优先级，如果运行A和B出现死锁，优先级低的回滚，优先级高的先执行，这样即可解决死锁问题。
    
    
#### 6、    存储引擎的 InnoDB 与 MyISAM
    MyISAM存储引擎的主要特点是：表级锁、不支持事务和全文索引，适合一些CMS内容管理系统作为后台数据库使用，但是使用大并发、重负荷生产
    系统上，表锁结构的特性就显得力不从心。
    
    InnoDB存储引擎的特点是：行级锁，事务安全（ACID兼容），支持外键、不支持FULLTEXT类型的索引（5.6.4之后开始支持FULLTEXT类型索引）。
    InnoDB是为处理巨大量时拥有最大性能而设计的。他的CPU效率可能是任何其他基于磁盘的关系数据库引擎所不能匹敌的。
    
#### 7、    数据库索引的原理
    索引的目的在于提高查询效率，可以类比字典。通过不断缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，
    就是我们总是通过同一种查找方式来锁定数据。
    
    数据库的情况要复杂很多，不仅面临等值查询，还有范围查询(>、<、between、in)、模糊查询(like)、并集查询(or)等等。搜索树的平均复杂度
    为logN，具有不错的查询性能，但是复杂度模型是基于每次相同的操作成本考虑的，数据库实现比较复杂，数据保存在磁盘上，而为了提高性能，每
    次又可以把部分数据读入内存来计算，因为访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足应用场景。
    
    磁盘读取数据量靠的是机械运动，每次读取数据花费的时间可分为寻道时间、旋转延迟、传输时间三个部分。寻道时间指的是磁臂移动到指定磁道所
    需的时间，主流磁盘一般在5ms以下，旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘7200转，表示每分钟能转7200次，也就是说1秒钟能转
    120次，旋转延迟就是1/120/2 = 4.17ms；传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略
    不计。那么访问一次磁盘的时间，即一次磁盘IO的时间约等于5+4.17 = 9ms左右，听起来还挺不错的，但要知道一台500 -MIPS的机器每秒可以执
    行5亿条指令，因为指令依靠的是电的性质，换句话说执行一次IO的时间可以执行40万条指令，数据库动辄十万百万乃至千万级数据，每次9毫秒的
    时间，显然是个灾难。
    
    考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址数据，而且吧相邻磁盘地质数据也读取到缓冲区
    内，因为局部预读性原理告诉我们，当计算机访问一个地址数据的时候，与其相邻的数据也会很快被访问到，每一次IO读取的数据我们称之为一页，
    具体一页有多大数据跟操作系统有关，一般为4K或8K，也就是我们读取一页内的数据的时候，实际上才发生了一次IO，这个理论对于索引的数据设计
    结构设计非常有帮助。
    
    1、IO的次数取决于b+树的高度h，假设当前数据表的数据为N，每个磁盘块数据项的数量是m，则有h=log(m+1)N,
    当数据量N一定的情况下，m越大， h越小，而m=磁盘块大小/数据项大小，磁盘块的大小也就是一个数据页的大小
    ，是固定的。如果数据项占的空间越小，数据项的数量就越多，树的高度就越低，这就是为什么每个数据项，
    即索引字段要尽量的小，比如int占4个字节，要比bigint8字节少一半。这也是为什么B+树要求把真是的数据放到
    叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅下降，导致树增高，当数据项等于1时，将
    退化成线性表。
    
    2、当B+树的数据项是复合的数据结构比如（name，age，sex）的时候，B+树是按照从左到右的顺序来建立搜索树
    的，比如当（张三，20，F）这样的数据来检索的时候，B+树会优先比较name来确定下一步的搜索方向，如果name
    相同再依次比较age和sex，最后得到检索的数据，但当（20，F）这样没有name的数据的时候，B+树就不知道下一
    步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去
    哪里查询，比如当（张三，F）这样的数据来检索时，B+树可以用name来指定搜索方向，但下一个字段age缺失，
    所以只能把名字等于张三的数据都找到，然后在匹配性别是F的数据，这个是非常重要的性质，即索引的最左匹配
    特性。
    
#### 8、    为什么要用 B-tree
    B-TREE检索一次最多需要访问h个节点，数据库系统的设计者巧妙的利用了磁盘的预读原理，将一个节点的大小设为
    等于一个页，这样每个节点只需要一次IO就可以完全载入，为了达到这个目的，在实际实现B-TREE还需要使用如下
    技巧：
    1、每次新建节点时，直接申请一个页空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都
    是按页对齐的，就实现了一个node只需一次IO。
    
    2、B-TREE中一次检索最多需要h-1次IO(根节点常驻内存)，渐进复杂度为O(h)=O(logdN)，一般实际应用中，出
    度d是非常大的数，通常超过100，因此h非常小，通常不超过3
    
    综上所述，用B-TREE作为索引结构效率是非常高的。
    
    而红黑树这种结构，h明显要深的多，由于逻辑上很近的点（父子）物理上可能很远，无法利用局部性，所以红黑树的
    渐进复杂度也为O（h），效率明显比B-TREE要小很多。
    
    上文还说过，B+TREE更适合外表索引，原因和内节点出度d有关，从上面分析可以看到，d越大索引的性能越好，而出
    度的上限取决于节点内key和data的大小：
    dmax= floor(pagesize/(keysize+datasize+pointsize))
    
    floor表示向下取整，由于B+TREE 内节点去掉了data域，因此可以拥有更大的出度，拥有更好的性能。

    
#### 9、    聚集索引与非聚集索引的区别
    MyISAM引擎使用B+TREE作为索引结构，是非聚集索引，如果指定的key存在，则取出其data域的值，然后以data域的
    值为地址，读取相应数据记录。
    
    InnoDB用的是聚集索引：
    1、InnoDB的数据文件本身就是索引文件，MyISAM的索引文件和数据文件是分离的，索引文件仅保存数据记录的地址，
    而在InnoDB中，表数据文件本身就是按照B+Tree组织的一个索引结构，这棵树的叶子节点data域保存了完整的数据
    记录，这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。
    
    2、InnoDB叶子节点包含了完整的数据记录，这种索引叫做聚集索引，因为InnoDb的数据文件本身要按主键索引，所
    以InnoDB必须要有主键。，如果没有显示指定，mysql系统会自动选择一个可以唯一标识的列做主键，如果不存在，
    这种列，则mysql自动为InnoDB生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整型。
    
    3、InnoDB的辅助索引data域存储相应记录主键的值而不是地址，换句说话，InnoDB的所有辅助索引都引用主键作为
    data域。
    
    所有的辅助索引都引用主索引，过长的主索引会另辅助索引变得过大，再例如用非单调的字段作为主键InnoDB中不是
    个好主意，因为InnoDB数据文件本身是B+TREE，非单调的主键会造成插入新纪录时数据文件为了维持B+TREE的特性
    而频繁分裂调整，十分低效，而使用自增字段作为主键则是一个好选择。
    
#### 10、    limit 20000 加载很慢怎么解决
    limit 20000 意思是抛弃前面的20000行返回后面的需要的行。
    
    可以记录上次查询的最大ID
    
    当一个数据库表过于庞大，limit offset，length中的offset值过大，则sql查询语句会非常缓慢，你需要增加
    order by，并且order by字段需要建立索引。
    如果使用子查询去优化limit的话，则子查询必须是连续的，某种意义讲，子查询不应该有where条件，where会
    过滤数据，使数据失去连续性。
    如果你查询记录比较大，并且数据传输量比较大，比如包含了text类型的field，则可以建立子查询。
    
    select id，title，content from items where id in （select id from items order by id limit 
    90000,10）
    
    如果limit offset较大，你可以通过传递pk键来减小offset=0；这个主键最好是int类型并且auto_increment
    
    
    1.子查询优化法
    先找出第一条数据，然后大于等于这条数据的id就是要获取的数据
    
    缺点：数据必须是连续的，可以说不能有where条件，where条件会筛选数据，导致数据失去连续性
    select * from Member where MemberID >= (select MemberID from Member limit 100000,1) limit
     100   
    
    
    2、使用 id 限定优化
    假设数据表的id是连续递增的，则我们根据查询的页数和查询的记录数可以推算出查询的id范围，可以使用id 
    between and 来查询。
    select * from orders_history where type=2 and id between 1000000 and 1000100 limit 100;  
    这种查询方式极大优化查询速度，基本能够在几十毫秒之内完成。限制是只能使用于明确知道id的情况，不过一般
    建立表的时候，都会添加基本的id字段，这为分页查询带来很多遍历。
    
    还可以使用in方式来进行查询，这种方式经常用在多表关联的时候进行查询，使用其他表的id集合来进行查询
    select * from orders_history where id in  
        (select order_id from trade_2 where goods = 'pen')  
    limit 100;  
    
    这种 in 查询的方式要注意：某些 mysql 版本不支持在 in 子句中使用 limit。 
    
    倒排表优化法类似建立索引，用一张表来维护页数，然后通过高效的连接得到数据
    缺点：只适合数据固定的情况，数据不能删除，维护页表困难。
    
    3、反向查找优化法
    
    当偏移超过一半记录数的时候，先用排序，这样偏移就反转了。
    缺点：order by优化比较麻烦，要增加索引，索引影响数据的修改效率，并且要知道总记录数，偏移大于数据的一半。
    
    4、limit限制优化法
    把limit偏移量限制低于某个数，超过这个数等于没数据
    
    5、只查索引法。
    
#### 11、    选择合适的分布式主键方案
    1、通过应用程序生成一个GUID，然后和数据一起插入切分后的集群。
    
    有电视维护简单，实现也容易，缺点是应用的计算成本大，且GUID的长度较长，占用数据库存储空间较大，涉及到应用
    开发
    
    2、通过独立的应用程序事先在数据库中生成一系列唯一的ID，各应用程序通过接口或者自己去读取再和数据一起插入到
    切分后的集群中。
    优点：全局唯一，维护相对容易，缺点：事先复杂需要应用开发。
    
    3、通过【中心数据库服务器】利用数据库自身的自增类型（如 MySQL的 auto_increment 字段），或者自增对象（如
     Oracle 的 Sequence）等先生成一个唯一 ID 再和数据一起插入切分后的集群。
     
    4、Twitter的snowflake算法
    这种方案生成一个64bit的数字，64bit被划分成多个段，分别表示时间戳、机器编码、序号。 
    
    
    ID为64bit 的long 数字，由三部分组成：
    
    41位的时间序列(精确到毫秒，41位的长度可以使用69年)。
    10位的机器标识(10位的长度最多支持部署1024个节点)。
    12位的计数顺序号(12位的计数顺序号支持每个节点每毫秒产生4096个ID序号)。
    优点：
    时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序。
    性能高，每秒可生成几百万ID。
    可以根据自身业务需求灵活调整bit位划分，满足不同需求。
    缺点：
    依赖机器时钟，如果机器时钟回拨，会导致重复ID生成。
    在单机上是递增的，但是由于涉及到分布式环境，每台机器上的时钟不可能完全同步，有时候会出现不是全局递增的情况。
    
    4、 TDDL序列生成方式
    TDDL是阿里的分库分表中间件，它里面包含了全局数据库ID的生成方式，主要思路：
    
    1、使用数据库同步ID信息
    2、每次批量获取一定数量可用的ID在内存中，使用完后，重新获取数据库获取下一批可用ID，每次获取的ID数量有步长
    控制，实际业务中可根据使用速度进行配置。
    
    3、每个业务可以给自己的序列起个唯一的名字，隔离各个业务系统的ID。
    
    
    
    5、Flicker方案
    
    主要采用mysql自增长的机制（auto_increment+replace into）
    replace into 跟 insert 功能类似，不同点在于：replace into 首先尝试插入数据到表中，如果发现表中已经有此
    行数据(根据主键或者唯-索引判断)则先删除此行数据，然后插入新的数据， 否则直接插入新数据。
    
    
    为了避免单点故障，最少需要两个数据库实例，通过区分auto_increment的起始值和步长来生成奇偶数的ID。
    
    
###    缓存使用

#### 1、Redis 有哪些类型
    1、String
    string为最简单的类型，一个key对应一个value
    set mykey "xxx"       设置key，第二次赋值会直接覆盖之前的
    setnx mykey "xxx"     如果mykey存在，则不改变，如果不存在，则创建赋值
    get mykey             获取key的值
    setex mykey 10 1      给mykey设置过期时间为10s，值为1
    mset key1 value1 key2 value2  设置多个key
    mget key1 key2        获取多个key值
    
    
    2、list
    list是一个链表结构，主要功能是push、pop以及获取范围的所有值等
    
    使用list结构，可以轻松实现最新消息排行，另一个应用是消息队列，可以利用list的push操作，将任务存在list中，
    然后工作线程再用pop操作将任务取出进行执行。（先进后出）
    
    lpush list1 "xxx"  在列表中加入元素
    lrange list1 0 -1  查看list1里面的所有元素
    lpop list1         取出list1最新的元素
    linsert list1 before "xxx" "xxx" 在值为xxx的前面插入一个元素为xxx
    lset list1 3 "xxx"  index为3的元素值修改为xxx
    lindex list1 0      查看第一个元素
    llen   list1        查看列表有多少个元素
    
    
    3、set
    set是集合，对集合操作有添加删除元素，有对多个集合求交并差等操作，在微博应用中，可以将一个用户关注的所有人放到
    一个集合里，将所有粉丝放在一个集合里，因为redis提供了求交集、并集、差集等操作，就可以方便的实现共同关注，共同
    喜好等功能。
    
    sadd set1 a b c d        创建set1并设置值
    smembers set1            查看集合set1的值
    srem set1 a b            删除set1的值
    spop set1                随机取出一个元素删除
    sinter set1 set2         交集
    sinterstore set1 set2 set3   把交集存到set3
    sunion set1 set2             并集
    sunionstore set1 set2 set3   把并集存到set3
    sdiff set1 set2              差集
    sdiffstore set1 set2 set3    把差集存到set3
    sismember set1 c             判断一个元素是否属于一个集合
    srandmember set1             随机取出一个元素，但不删除。
    
    4、sorted set 
    sorted set 是有序集合，比set多了一个权重参数score，使得集合元素能够按照score进行有序排列。
    
    例如存储一个班级的同学成绩，其集合value可以是同学的学号，而score可以是其考试得分，这样在数据插入集合的时候就进行了
    排序
    
    zadd zset1 1 a    增加一个zset1，score为1，member为a
    zrange zset1 0 -1  按score升序输出member
    zrange zset1 0 -1 withscores  带上分值
    zrem zset1 a      删除指定元素
    zrank zset1 a     返回元素的索引值，索引从0开始
    zrevrange zset1 0 -1  按score降序输出member
    zcard zset1        返回集合中所有元素的个数
    zcount zset1 1 10  返回分值范围为1-10的元素个数
    zrangebyscore zset1 1 10  返回分值范围1-10的元素
    zremrangebyscore zset1 1 10 删除分值范围1-10的元素
    
    
    5、hash
    把一些结构化的信息打包成hashmap，在客户端序列化后存储为一个字符串的值（一般为json格式），比如用户名，年龄，性别等
    
    hset hash1 name "xxx"    建立hash（hset name key value）
    hget hash1 name          获取field值 HGET name key
    hgetall hash1            获取hash1中所有的key和value
    hmset hash2 name xxx age 26 job it  批量建立键值对
    hmget hash2 name age job   批量获取field值
    hdel hash2 job             删除指定field
    hkeys hash2                打印所有的key
    hvals hash2                打印所有的value
    hlen hash2                 查看hash2有几个field 
    
    
#### 2、    Redis 内部结构
    RedisObject核心对象，Redis内部使用一个RedisObject对象来表示所有的key-value
    主要属性有：
    1、type：代表一个value对象具体是何种数据类型
    2、encoding：是不同数据类型在redis内部的存储方式，比如type=string代表value存储的是一个普通字符串，那么对应
    的encoding可以是raw或者是int，如果是int则代表实际redis内部是按数值类存储和表示这个字符串的，当然前提是这个
    字符串本身可以用数值表示，比如：“123” 这样的字符串。
    
    3、vm：只是打开了Redis内部的虚拟内存功能，此字段才会真正分配内存，该功能默认是关闭状态，Redis使用RedisObject
    来表示所有的key/value是比较浪费内存的，当然这些内存管理成本的付出主要也是为了给Redis不同数据类型提供一个统一的
    管理接口，实际作者也提供了多种方法帮助我们尽量节省内存使用。
    
    key（键值）：
    过期删除：过期数据的清除从来不容易，为每一条key设置了一个timer，到点立刻删除的消耗过大，每秒遍历所有数据消耗也大，
    Redis使用了一种相对务实的做法:当client主动访问key，会先对key进行超时判断，过时的key会立刻删除。如果client永远
    都不再get那条key呢？他会在master后台，每秒10次，随机选取100个key校验是否过期，如果超过25个key过期了，立刻额外
    随机取下100个key（不计算在10次之内）。可见，如果过期的key不多，他最多每秒回收200条左右，如果有超过25%的key过期
    了，他就会做得更多，但只要key不被主动get，他占用的内存也不会知道什么时候被清理。
    
    常用操作：
    1、key的长度限制，key的最大长度不能超过1024字节，在实际开发时不要超过这个长度，但是key的命名不能太短，要能唯一
    标识缓存的对，作者建议按照在关系型数据库中的库表唯一标识字段的方式来命令key的值，用分号分割不同的数据域，用点号
    作为单词连接。
    
    2、key的查询：keys，返回匹配的key，支持通配符keys a*、keys a?c 但不建议在生产环境大数据量下使用。
    
    3、对key对应的value进行的排序：Sort命令对集合按数字或字母顺序排序后返回或另存为list，还可以关联到外部key等
    因为复杂度是最高的O（N+Mlog（M））（N是集合大小，M为返回元素的数量），有时会安排到slave上执行。
    
    4、key的超时操作：Expire（指定失效的秒数），ExpireAt（指定失效的时间戳），Persist（持久化），TTL（返回还可存活
    的秒数），关于key的超时操作，默认以秒为单位，也有p字头的以毫秒为单位的版本。
    
    
    string  字符串类型value
    可以是string也可以是任意的byte[]类型数组。如图片等，String在redis内部存储默认就是一个字符串，被redisObject所引
    用，当遇到incr，decr等操作时，会转成数值型进行计算，此时RedisObject的encoding字段为int
    
    1、大小限制：最大为512Mb，基本可以存储任意图片
    
    2、常用命令的时间复杂度为O（1），读写一样快
    
    3、对String代表的数字进行增减操作（没有指定key则设置为0值，然后再进行操作）：incr/incrBy/incrbyfloat/decr/
    decrBy(原子性)，可以用来做计数器，做自增序列，也可用于限流，令牌桶计数等，可以不存在时会创建并贴心的设原值为0，
    incrbyfloat专门针对float。
    
    4、设置value的安全性：setNX命令仅当key不存在时才set（原子操作），可以用来选举master或做分布式锁，所有client不断尝
    试使用setNX master myName抢注master，成功的那位不断使用expire刷新他的过期时间，如果master倒掉了key就会失效，剩下的
    节点又会发生新一轮的抢夺，SetNX,set +expire的简便写法，p字头版本为毫秒单位。
    
    5、Get Set（原子性），设置新值，返回旧值，比如一个按小时计算的计数器，可以用GetSet获取技术并重置为0，这种指令在服务端
    做起来是举手之劳，客户端便方便很多。MGet/MSet/MSetNx,一次get、set多个key。
    
    hash（hashmap，哈希映射表）
    redis的hash实际是内部存储的Value为一个hashmap，并提供了直接存取这个map成员的接口，hash将对象各个属性存入map里，可以
    只读取、更新对象的某些属性，另外不同的模块可以只更新自己关心的属性而不会互相并发覆盖冲突。
    
    不同程序通过key（用户ID）+field（属性标签）就可以并发操作各自关心的属性数据。
    
    实现原理：
    Redis hash对应value内部实际就是一个hashmap，实现这里有2种不同实现，这个hash的成员较少时，Redis为了节省内存会采用类似
    一堆数组的方式来紧凑存储，而不会采用真正的hashmap结构，对应的value redisObject的encoding为zipmap，当成员数量增大时
    会自动转成真正的hashmap，此时encoding为ht。一般操作复杂度是O（1），要同时操作多个field时就是O（N），N是field数量。
    
    
    list（双向链表）
    Redis list的应用场景非常多，也是Redis最重要的数据结构之一，比如twitter的关注列表，粉丝列表等可以用Redis的list结构来
    实现，还提供了生产者消费者阻塞模式（B开头的命令），常用于任务队列，消息队列等。
    
    实现方式：
    Redis list的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销，Redis内部的很多实现
    包括发送缓冲队列等也都是用的这个数据结构。
    
    set（hashset）
    set就是hashset，可以将重复的元素随便放入而set会自动去重，底层实现也是HashMap，并且set提供了判断某个成员是否在一个
    set集合内的重要接口，这个也是list所不能提供的。
    
    实现原理：
    set的内部实现是一个value永远为null的hashmap，实际就是通过计算hash的方式来快速排重，这也是set能提供判断一个成员是否
    在集合内的原因。
    
    
    sorted set（插入有序set集合）
    set不是自动有序的，而sorted set可以通过用户额外提供一个优先级（score）的参数来为成员排序，并且是插入有序的，即
    自动排序，当你需要一个有序并且不重复的集合列表，那么可以选择sorted set数据结构，比如twitter的public timeline
    可以发表时间作为score来存储，这样获取时就是自动按时间拍好序的。
    
    实现方式：
    内部试用hashmap和跳跃表（skiplist）来保证数据的存储和有序。
    
    sorted set的实现是hashmap（element -> score，用于实现zscore以及判断element是否在集合内）和skiplist（score
    ->element，按score排序）的混合体。SkipList有点像平衡二叉树那样，不同范围的score被分成一层一层，每层是一个按sc
    ore排序的链表。
    
    总结：
    1、根据业务需要选择合适的数据类型，并为不同的应用场景设置相应的紧凑存储参数。
    
    2、当业务场景不需要数据持久化时，关闭所有的持久化方式可以获得最佳的性能以及最大的内存使用量。
    
    3、如果需要使用持久化，根据是否可以容忍重启丢失部分数据在快照方式与语句追加方式之间选择其一，不要使用虚拟内存以及
    diskstore方式
    
    4、不要让你的Redis所在机器物理内存使用超过实际内存容量的3/5.
    
    redis持久化使用buffer io,所谓buffer io是指redis对持久化文件的写入和读取操作都会使用物理内存的page cache，而当
    redis的持久化文件过大操作系统会进行swap，这时你的系统就会有内存还有余量但是系统不稳定或者崩溃的风险。
    
#### 3、    聊聊 Redis 使用场景
    随着数据量的增长，Mysql已经满足不了大型互联网类应用的需求。因此，Redis基于内存存储数据，可以极大的提高查询性能，
    对产品在架构上很好的补充，在某些场景下，可以充分利用REDIS特性，大达提高效率。
    
    缓存
    对于热点数据，缓存以后可能读取数十万次，因此对于热点数据，缓存的价值非常大，例如分类栏目更新频率不高，但是绝大多数
    的页面需要访问这个数据，因此读取频率相当高，可以考虑基于redis实现缓存。
    
    会话缓存
    将web session存放redis中
    
    时效性
    例如验证码只有60s有效期，超过时间无法使用，或者基于oauth2的token只能在5分钟内使用一次。超过时间也无法使用。
    
    访问频率
    出于减轻服务器压力或防止恶意的洪水攻击的考虑，需要控制访问频率，例如限制Ip在一段时间的最大访问量。
    
    计数器
    数据统计的需求非常普遍，通过原子递增保持计数，例如应用数，资源数，点赞数，收藏数，分享数等。
    
    社交列表
    社交属性相关的列表信息，例如用户点赞列表，用户分享列表，用户收藏列表，用户关注列表，用户分析列表，使用hash
    类型数据结构是个不错的选择。
    
    记录用户判定信息
    记录用户判定信息的需求也非常普遍，可以知道一个用户是否进行了某个操作，例如，用户是否点赞，用户是否收藏，用户
    是否分享等。
    
    交集、并集、差集
    在某些场合，例如社交场景，通过交集、并集和差集运算，可以非常方便地实现共同好友，共同关注，共同偏好等社交关系。
    
    热门列表与排行榜
    按照得分进行排序，例如展示最热，点击率最高，活跃度最高 等条件的排名列表。
    
    最新动态
    按照时间顺序排列的最新动态，也是一个很好的应用，可以用sorted set类型的分数权重存储unix时间戳进行排序。
    
    消息队列
    Redis能作为一个很好的消息队列来使用，以来list 类型利用Lpush命令将数据添加到链表头部，通过BRPOP命令将元素从链表
    尾部取出，同时，市面上成熟的消息队列产品有很多，例如RabbitMQ，因此，更加建议使用RabbitMQ作为消息中间件。
    
    
#### 4、    Redis 持久化机制
    Redis提供了RDB持久化和AOF持久化。
    RDB:
    RDB持久化是值在指定时间间隔内将内存中的数据集快照写入硬盘。
    也是默认的持久化方式，这种方式将内存中的数据以快照的方式写入到二进制文件中，默认文件名为dump.rdb。
    可以通过配置设置自动做快照持久化的方式，我们可以配置redis在n秒内如果超过m个key被修改就自动做快照，
    下面是默认的快照保存配置：
    save 900 1     #900秒内如果超过1个key被修改，则发起快照保存
    save 300 10    #300秒内容如超过10个key被修改，则发起快照保存
    save 60 10000
    
    
    RDB文件保存过程
    1、redis调用fork，现在有子进程和父进程
    2、父进程继续处理client请求，子进程负责将内容写入到临时文件。由于OS的写时复制机制（copy on write）父子进程
    会共享相同的物理页面，当父进程处理写请求时OS会为父进程要修改的页面创建副本，而不是写共享的页面，所以子进程
    的地址空间内的数据是fork时刻整个数据库的一个快照。
    3、当子进程将快照写入临时文件完毕后，用临时文件替换原来的快照文件，然后子进程退出。
    
    client也可以通过save或者bgsave命令通知redis做一次快照持久化。save操作是在主线程中保存快照的，由于redis是用
    一个主线程来处理所有client的请求，这种方式会阻塞所有client的请求。所以不推荐使用。
    
    另一点需要注意的是，每次快照持久化都是将内存数据写入到磁盘一次，并不是增量的只同步脏数据，如果数据量大的话，
    而且写操作比较多，必然会引起大量的磁盘IO操作，可能严重影响性能。
    
    
    优势：
    1、一旦采用该方式，namezhenggeredis数据库将只包含一个文件。这样非常方便的进行备份。
    2、RDB在恢复大数据集的时候速度比AOF的恢复速度要快。
    3、RDB可以最大化REDIS的性能，父进程保存RDB文件时唯一要做的就是fork出一个子进程，然后这个子进程就会处理接下
    来的所有保存工作，父进程无需执行任何磁盘IO操作。
    
    劣势：
    1、如果你需要 尽量避免在服务器故障时丢失数据，那么RDB不适合你，虽然Redis允许你设置不同的保存点（save point）
    来控制保存RDB文件的频率，但是，因为RDB文件需要保存整个数据集的状态，所以他并不是一个轻松的操作，因此你可能
    会至少5分钟才保存一次RDB文件，在这种情况下，一旦发生故障停机，就可能会丢失好几分钟的数据。
    
    2、每次保存RDB的时候，Redis都要fork（）出一个子进程，并由子进程进行实际的持久化工作，在数据集比较庞大时，fork（）
    可能会非常耗时，造成服务器在某某毫秒内停止处理客户端，如果数据集非常巨大，并且cpu时间非常紧张的话，那么这种停止
    时间甚至可能长达整整一秒，虽然AOF重写也需要进行fork（），但无论AOF重写的执行间隔有多长，数据的耐久性都不会有
    任何损失。
    
    
    
    AOF文件保存过程：
    redis会将每一个收到的写命令都通过write函数追加到文件中（默认是appendonly.aof）
    
    当redis重启时，会通过重新执行文件中的保存的写命令来在内存中重建整个数据库的内容，当然由于os会在内核中缓存write时的
    修改，所以可能不是立刻写到磁盘上，这样aof方式的持久化也还是有可能会丢失部分修改。不过我们可以通过配置文件告诉redis
    我们想要通过fsync函数强制os写入到磁盘的时机，有三种方式（默认是每秒fsync一次）
     appendonly yes              //启用aof持久化方式
     # appendfsync always      //每次收到写命令就立即强制写入磁盘，最慢的，但是保证完全的持久化，不推荐使用
     appendfsync everysec     //每秒钟强制写入磁盘一次，在性能和持久化方面做了很好的折中，推荐
     # appendfsync no    //完全依赖os，性能最好,持久化没保证
     
     
    aof的方式也同时带来了另一个问题，持久化文件会变得越来越大，例如我们调用incr test命令100次，文件中必须保存全部的100
    条命令，其实有99条是多余的，因为回复数据库的状态只需要保存一条set test 100 就够了。
    
    为了压缩AOF的持久化文件，redis提供了bgrewriteaof命令，收到此命令将使用与快照类似的方式将内存中的数据以命令的方式保存
    到临时文件中，最后替换原来的文件，具体过程如下：
    1、redis调用fork，现在又父子两个进程。
    2、子程序根据内存中的数据库快照，往临时文件中写入重建数据库状态的命令。
    3、父进程继续处理client请求，除了把命令写入原来的aof文件中，同时把收到的写命令缓存起来，这样就能保证如果子进程重写
    失败的话并不会出现问题。
    4、当子进程把快照内容写入已命令方式写到临时文件中后，子进程发信号通知父进程，然后父进程把缓存的命令写入到临时文件。
    5、父进程使用临时文件替换老的AOF文件，并重命名，后面收到的写命令也开始往新的aof中追加。
    
    需要注意的是重写AOF文件的操作，并没有读取旧的aof文件，而是将整个内存中的数据库内容用命令的方式重写了一个新的aof文件
    这点和快照有些类似。
    
    优势：
    1、使用aof持久化会让redis变得非常耐久，可以设置不同的fsync策略，比如无fsync，每秒钟一次fsync或者每
    次执行写入命令时fysnc，aof的默认策略为每秒钟处理一次，在这种配置下，redis仍然可以保持良好的的性能
    并且就算发生故障停机，也最多只会丢失一秒钟的数据（fsync会在后台线程执行，所以主线程可继续努力处理
    命令请求）
    
    
    2、AOF文件是一个只进行追加操作的日志文件（append only log），因此对AOF文件的写入不需要进行seek，即使
    日志因为某些原因而包含了未写入完整的命令（比如写入时磁盘已满，写入中途停机），redis-check-aof工具也可以
    轻易地修复这种问题。redis可以在aof文件体积变得过大时，自动地在后台对AOF进行重写，重写后的新AOF文件包含了
    恢复当前数据集所需的最小命令集合，整个写操作是绝对安全的，因为redis在创建新aof文件的过程中，会继续将命令
    追加到现有的AOF文件里面，即使重写过程中发生停机，现有的aof文件也不会丢失，而一旦新aof文件创建完毕，redis
    就会从旧aof文件切换到新的aof文件，并开始对新aof文件进行追加操作。
    
    
    3、aof文件有序地保存了对数据库执行的所有写入操作，这些写入操作以redis协议的格式保存，因此AOF文件的内容非常
    容易被人读懂，对文件进行分析（parse）也很轻松，导出（export）AOF文件也非常简单例如，如果不小心执行了flushall
    命令。但只要AOF文件未被重写，那么只要停止服务器，移除AOF文件末尾的FLASHALL命令，并重启REDIS，就可以将数据集
    恢复到FLUSHALL执行之前的状态。
    
    劣势：
    1、对于相同的数据集来说，AOF文件的体积通常要大于RDB文件的体积。
    
    2、根据所使用的fysnc策略，AOF的速度可能会慢于RDB，在一般情况下，每秒fsync的性能依然 非常高，而关闭fsync
    可以让AOF的速度和RDB一样快，即使在高负荷下，也是如此，不过在处理巨大的写入载入时，RDB可以提供更有保证的
    的最大延迟时间（latency）
    
    3、aof在过去曾经发生过这样的bug：因为个别命令的原因，导致aof文件在重载入时，无法将数据集恢复成保存时的原样，
    例如（阻塞命令BRPOPLPUSH就曾经引发这样的bug），测试套件里为这种情况，添加了测试，它们会自动生成随机的、复杂
    的数据集，并通过重新载入这些数据确保一切正常。虽然这种bug在aof文件中不常见，但对比来说，RDB几乎不可能出现这
    样的bug。
    
    
    抉择：
    一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。
    
    如果你非常关心你的数据，但仍然可以承受数分钟以内的数据丢失。那么可以使用RDB持久化
    
    其余情况选择AOF。
    
    
    
#### 5、Redis 为什么是单线程的    
    因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽，既然单线程
    容易实现，而且cpu不会成为瓶颈，那就顺理成章的采用单线程了（毕竟采用多线程会有很多麻烦，避免使用锁）
    
    正式由于在单线程模式的情况下已经很快了，就没有必要使用多线程了，但是，我们使用单线程的方式是无法发挥多核
    cpu性能，不过我们可以通过在单机开多个Redis实例来完善。
    
    注意：这里一直强调的单线程，只是在处理我们的网络请求的时候只有一个线程来处理，正式的redisserver运行时肯定
    不止一个线程。
    
    
    redis快原因
    1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速，数据在内存中，类似于hashmap，hashmap的优势就是
    查找和操作的时间复杂度都是O（1）
    
    2、数据结构简单，对数据操作简单，redis的数据结构是专门设计的。
    
    3、采用单线程避免不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗CPU，不用去考虑
    各种锁问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗。
    
    4、使用多路i/o复用模型，非阻塞IO
    
    5、使用底层模型不同，他们之间实现方式以及客户端之间通信的应用协议不一样，Redis直接自己构建了VM机制，
    因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。
    
    
#### 6、    缓存崩溃
    缓存雪崩：
    缓存雪崩是由于原有缓存失效（过期），新缓存未到期间，所有请求都去查询数据库，而对数据库CPU和内存造成巨大压力，
    严重的会造成数据库宕机，从而形成一系列连锁反应，造成整个系统崩溃。
    
    1、碰到这种情况，一般并发量不是特别多的时候，使用的最多的解决方案是加锁排队。
    
    2、加锁排队只是为了减轻数据库的压力，并没有提高系统吞吐量，假设在高并发下，缓存重建期间key是锁着的，这时过来
    1000个请求，999个堵塞，同样会导致用户等待超时，这个治标不治本。
    
    还有一个解决方案是：给每一个缓存数据增加相应的缓存标记，记录缓存是否失效，如果缓存标记失效，更新数据缓存。
    
    缓存标记：
    记录缓存数据是否过期，如果过期会触发通知另外的线程在后台去更新实际key的缓存。
    
    缓存数据：
    他的过期时间比缓存标记的时间延长一倍，例：标记缓存时间30分钟，数据缓存设置为60分钟，这样，当缓存标记key过期后
    。实际缓存还能把旧数据返回给调用端，直到另外的线程后台更新完成后，才返回新的缓存，这样做后，可以一定成提高并
    发量。
    
    缓存穿透：
    缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有吗，这样就导致用户查询的时候，在缓存中找不到。每次
    都要去数据库再查询一遍，然后返回空，这样请求就绕过缓存直接查数据库，这是经常提的缓存命中率问题。
    
    解决办法是：如果查询数据库为空，直接设置一个默认值存放到缓存，这样第二次缓存中获取到就有值了，而不会继续访问数
    据库，这种办法简单粗暴。
    
    把空结果也给缓存起来，这样下次同样的请求就可以直接返回了空了，即可以避免当查询为空时引起的缓存穿透，同时也可以
    单独设置个缓存区域存储空值，对要查询的key预先校验，然后再放行给后面的正常缓存处理的逻辑。
    
    
    缓存预热：
    缓存预热就是系统上线后，将相关缓存数据直接加载到缓存系统，这样避免用户请求的时候再去加载相关的数据。
    
    解决思路：
    1、直接写个缓存刷新的页面，上线时手工操作下。
    2、数据量不大可以再web系统启动的时候加载。
    3、定时刷新缓存
    
    缓存更新：
    缓存淘汰的策略有两种：
    1、定时去清理过期的缓存。
    2、当用户请求过来时，判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。
    
    两者各有优劣，第一种缺点是维护大量缓存的key是比较麻烦的，第二种缺点就是每次用户请求过来要要判断缓存失效，逻辑
    相对比较复杂，具体用哪种方案，可以根据自己的场景去权衡。
    1、预估失效时间
    2、版本号（必须是单调递增，时间戳是最好的选择）
    3、提供手动清理缓存的接口。
    
####  7、缓存降级
    
    当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍要保证服务还是可用的，即使
    是有损服务。系统可以根据一些关键数据自行降级，也可以配置开关实现人工降级。
    
    降级的最终目的是保证核心服务可用，即使是有损的，而且有些服务是无法降级的（如加入购物车、结算）。
    
    在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅，从而梳理出哪些必须誓死保护，哪些可降级，比如可以
    参考日志级别设置预案，
    1、一般：比如有些服务偶尔因为网络抖动或者正在而超时可以自动降级
    
    2、警告：有些服务在一段时间内成功率有波动（如在95--100%之间），可以自动降级或人工降级，并发送警告
    
    3、错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阈值，此时可以根据
    情况自动降级或者人工干预降级
    
    4、严重错误：比如因为特殊原因数据错误了，此时紧急需要人工降级。
    
    
####  8、   使用缓存的合理性问题

    1、热点数据：
    对于冷数据而言，读取频率低，大部分数据可能还没有再次访问到就已经被挤出去，不仅占用内存，而且价值不大，
    对于热点数据而言，读取频率高，如果不做缓存，给数据库造成很大压力，可能被击穿。
    
    2、修改频率：
    数据更新前至少读取两次，缓存才有意义，这是基本策略，如果缓存还没有起作用就失败了，那就没有太大价值了。
    （读取频率>修改频率），如果这个读取接口对数据库压力很大，但是又是热点数据，这个时候就需要考虑通过缓存
    手段，减少数据库的压力。比如点赞数，收藏数，分享数等非典型的热点数据，但是又不断变化，此时就需要将
    数据同步保存到redis缓存减少数据库压力。
    
    3、缓存更新机制
    一般情况下，我们采用双淘汰机制，在更新数据库的时候淘汰缓存，此外，设定超时时间，例如30分钟，极限场景下
    即使有脏数据入cache，这个脏数据也最多存在30分钟，在高并发情况下，设计上最好避免查询mysql，所以在更新数
    据库的时候更新缓存。
    
    4、 缓存可用性
    缓存是提高数据读取性能的，缓存数据丢失或缓存不可用不会影响程序处理，因此，一般的操作手段是，如果redis
    出现异常，我们手动捕获这个异常，记录日志，并且去数据库查询数据返回给用户。
    
    5、服务降级
    服务降级的目的是为了防止redis服务故障服务故障，导致数据库跟着一起发生雪崩问题，因此，对于不重要的缓存
    数据，可以采取服务降级策略，例如一个比较常见的做法就是，redis出现了问题不去数据库查询而是返回默认的给用户。
    
    在大公司，redis都是codis集群，一般整个codis是不会挂掉的，所以程序代码上没法去实现可用性、服务降级
    
    6、缓存预热
    在新启动的缓存系统中，如果没有任何数据，在重建缓存数据过程中，系统的性能和数据库复制都不太好，那么最好
    缓存系统启动时就把热点数据加载好，例如对于缓存信息，在启动加载数据库中全部数据进行预热，一般情况下，会
    开通一个同步数据的接口进行缓存预热。
    
    
###    消息队列

#### 1、消息队列的使用场景
       
    消息队列中间件是分布式系统中重要的组件，主要解决应用解耦，异步消息，流量削峰等。
    
    实现高性能，高可用，可伸缩和最终一致性架构。
    
    使用较多的消息队列有ActiveMQ，RabbitMQ，ZreoMQ，Kafka，MetaMQ，RocketMQ
    
    应用场景：
    
    1、异步处理
    场景说明：用户注册后，需要发注册邮件和注册短信。传统的做法有两种 1.串行的方式；2.并行方式
    
    （1）串行方式：将注册信息写入数据库成功后，发送注册邮件，再发送注册短信。以上三个任务全部完成后，返回给
    客户端
    
    （2）并行方式：将注册信息写入数据库成功后，发送注册邮件的同时，发送注册短信。以上三个任务完成后，返回给
    客户端。与串行的差别是，并行的方式可以提高处理的时间
    
    
    引入消息队列，将不是必须的业务逻辑异步处理，用户响应的时间相当于注册信息写入数据库的时间也就是50ms，注册
    邮件，发送短信写入消息队列，直接返回，因此写入消息队列的速度更快，基本可以忽略，因此架构改变后，系统的吞
    吐量提高到每秒20万QPS，比串行提高了三倍。
    
    
    
    2、应用解耦
    场景说明：用户下单后，订单系统需要通知库存系统。传统的做法是，订单系统调用库存系统的接口
    
    引入消息队列后：
    订单系统：用户下单后，订单系统完成持久化处理，将消息写入消息队列，返回用户订单下单成功
    库存系统：订阅下单的消息，采用推/拉的方式获取下单消息，库存系统根据下单消息进程库存操作
    
    如果在下单时库存系统不能正常使用，也不影响正常下单，因为下单后，订单系统写入消息队列就不
    再关心其他的后续操作了，实现订单系统与库存系统的应用解耦。
    
    3、流量削峰
    流量削峰是消息队列中的常用场景，一般在秒杀和抢购活动中，
    
    应用场景：
    秒杀活动，一般会因为流量过大，导致流量暴增，应用挂掉，为解决这个问题，一般需要在应用前端加入消息队列
    可控制活动人数
    可以缓解短时间内高流量压垮应用。
    用户的请求，服务器接受后，首先写入消息队列，假如消息队列长度超过最大数量，则直接抛弃用户请求或跳转到错误页面
    秒杀业务根据消息队列中请求的信息，再做后续处理
    
    4、日志处理
    日志处理是指将消息队列用在日志处理中，比如kafka的应用，解决大量日志传输的问题
    
    1、日志采集客户端负责日志数据采集，定时写入kafka队列
    2、kafka消息队列，负责日志数据的接收，存储和转发
    3、日志处理应用：订阅并消费kafka队列中的日志数据。
    
    
#### 2、    如何保证消息的有序性
    要保持 多个消息之间的时间顺序，首先他们要有一个全局的时间顺序，因此每个消息在被创建时都被赋予一个
    全局唯一的、单调递增的、连续的序列号，可以通过一个全局计数器来实现这一点，通过比较两个消息的SN，确定其先后顺序。
    
#### 3、    消息的幂等性解决思路

    MQ为了保证消息必达，消息上下半场均可能发送重复消息，如何保证消息的幂等性
    
    上半场：
    1、MQ-CLIENT生成inner-msg-id，保证上半场幂等
    2、这个ID全局唯一，业务无关，由MQ保证
    
    下半场：
    业务发送方带入biz-id，业务接收方去保证幂等
    这个id对单业务唯一，业务相关，对MQ透明
    结论：幂等性，不仅对MQ有要求，对业务上下游也有要求。
    
#### 4、    消息的堆积解决思路
    处理消息堆积的方法就是把它保存起来，只是这个存储可以做成很多形式的，比如内存、分布式KV,磁盘，数据库等
    ，但归结起来有持久化和非持久化两种


    持久化的形式能更大程度地保证消息的可靠性，并且理论上能承受更大限度的消息堆积（外存的空间远大于内存）
    
    但并不是每种消息都需要持久化存储，很多消息对于传递性能的要求大于可靠性要求，且数量极大，这时候消息不落
    地直接暂存内存 ，尝试几次failover最终投递出去。

## 框架篇

### Spring

####  1、BeanFactory 和 ApplicationContext 有什么区别
    BeanFactory是Spring里面最底层的接口，提供了简单的容器功能，只提供了实例化对象和拿对象的功能。
    
    ApplicationContext：应用上下文，继承BeanFactory接口，是一个更高级的容器：
    1、国际化（MessageSource）
    2、访问资源，如URL和文件（ResourceLoader）
    3、载入多个（有继承关系）上下文，使得每一个上下文都专注于一个特定的层次，比如应用的web层
    4、消息发送、响应机制（ApplicationEventPublisher）
    5、AOP（拦截器）
    
    两者装载bean的区别
    
    BeanFactory在启动的时候不会去实例化Bean，只有从容器中拿Bean的时候才会去实例化
    
    ApplicationContext再启动的时候就把所有的Bean实例化了，还可以为Bean配置lazy-init=true来让bean延迟实例化
    
    BeanFactory延迟实例化的优点
    应用启动的时候占用资源很少，对资源要求较高的应用，比较有优势
    
    不延迟实例化的优点
    1、所有的Bean在启动的时候都加载，系统运行速度快
    
    2、再启动的时候的所有的Bean都加载了，我们能在系统启动的时候尽早发现系统中的配置问题
    
    3、建议WEB应用，在启动的时候把所有的Bean都加载了（把费时的操作放到系统启动中完成） 

    
#### 2、Spring Bean 的生命周期
    1、spring对bean进行实例化，默认Bean是单例的。
    2、spring对bean进依赖注入
    3、如果Bean实现BeanNameAware接口，spring将bean的id传给setBeanName（）方法
    4、如果Bean实现了BeanFacetoryAware接口，spring将调用setApplicationContext（）方法将应用上下文的引用传入
    5、如果Bean实现了ApplicationContextAware（）接口，spring将调用setApplicationContext（）方法将应用上下文导入
    6、如果Bean实现了BeanPostProcessor接口，spring将调用它们的postProcessBeforeInitialization接口方法
    7、如果Bean实现了InitializatingBean接口，spring将调用它们的afterPropertiesSet接口方法，类似的如果bean使用了
    init-method属性声明了初始化方法，该方法也会被调用。
    8、如果bean实现了BeanPostProcessor接口，spring将调用他们的postProcessAfterInitialization接口方法
    9、此时bean已经准备就绪了，可以被应用程序使用了，它们将一直驻留在应用上下文中，直到该应用上下文被销毁
    10、若bean实现了DisposableBean接口，spring将调用他的destroy（）接口方法，同样的，如果bean使用了destroy-method
    属性声明了销毁方法，则该方法被调用。
    
    我们可以对其进行分类：
    Bean自身的方法 ：配置文件中的init-method和destroy-method配置的方法、bean对象自己调用的方法
    Bean级生命周期接口方法：BeanNameAware、BeanFactoryAware、InitializingBean、DisposableBean等接口中的方法
    容器级生命周期接口方法：InstantiationAwareBeanPostProcessor、BeanPostProcessor等后置处理器实现类中重写的方法
    
    
    
    
#### 3、    Spring IOC 如何实现
    
    IOC:Inversion Of Control，即控制反转。是一种思想，原先需要自行实例化的对象，交给IOC容器去实现。
    
    在传统JavaSE程序中，直接在类的内部通过new关键字创建对象实例，是程序主动创建依赖对象，而IOC有一个专门容器，来创建
    这些对象，由IOC容器来控制对象的创建，依赖对象也是容器帮忙查找创建并进行注入，对象只是被动的接受，依赖对象的获取被
    反转了。还有一个更形象的是依赖注入
    
    注入方式：
    1、接口注入
    2、setter
    3、构造器注入
    
    IOC容器的设计与实现有两种：BeanFactory 和 ApplicationContext
    
    
    
####  4  、说说 Spring AOP
    AOP ：Aspect Oriented Programming 面向切面编程，通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术
    AOP是OOP的延续，是软件开发中的一个热点，也是Spring框架中的一个重要内容，是函数式编程的一种衍生泛型，利用AOP可以
    对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。
    常用于日志记录，性能统计，安全控制，事务处理。异常处理等。
    
    定义AOP术语：
    切面Aspect：切面是一个关注点模块化，这个关注点可能是横切多个对象。
    
    连接点Join Point：连接点是指在程序执行过程中某个特定的点，比如某个方法调用的时候或者处理异常的时候。
    
    通知Advice：是指在切面的某个特定的连接点上执行的动作，Spring切面可以应用5种通知：
    前置通知
    后置通知
    返回通知
    异常通知
    环绕通知
    
    
    切点PointCut 指匹配连接点的断点，通知与一个切入点表达式关联，并在满足这个切入的连接点上运行，例如当
    执行某个特定的名称的方法。
    
    引入Introduction：引入也被称为内部类型声明，声明额外的方法或者某个类型的字段。
    
    目标对象Target Object：目标对象是被一个或者多个切面所通知的对象。
    
    AOP代理：AOP代理是指AOP框架创建的对象，用来实现切面契约（包括通知方法等功能）
    
    织入（Wearving）：指把切面连接到其他应用程序类型或者对象上，并创建一个被通知的对象，或者说形成代理对象
    的方法的过程
    
    
#### 5、    Spring AOP 实现原理
    SpringAOP主要使用动态代理，AOP框架不会去修改字节码，而是在内存中临时为方法生成一个AOP对象，这个对象
    包含了目标对象的的全部方法，并且在特定的切点做了增强处理，并回调对象的方法。
    
    SpringAOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理。JDK动态代理通过反射来接受被代理的类
    并且要求被代理的类实现一个接口，JDK动态代理的核心是InvocationHandler接口和Proxy类。
    
    如果目标类没有实现接口，那么SpringAOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Libray）
    是一个动态生成代码的类库，可在运行时生成某个类的子类，注意，CGLIB通过继承的方式做的动态代理，如果某个
    类被标记为final，那么他是无法使用CGLIB做动态代理的。
    
    Aspectj在编译时就增强了目标对象，SpringAOP的动态代理则是在每次运行时动态的增强，生成AOP代理对象，区别
    在于生成AOP代理对象的时机不同，相对来说AspectJ的静态代理方式具有更好的的性能，但是Aspectj需要特定的编
    译器进行处理，而SpringAOP则无需特定的编译处理。
    
    
    
#### 6、    Spring 事务实现方式
    1、事务的作用
    将若干数据库操作作为一个整体控制，一起成功或一起失败
    原子性：指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生
    一致性：指事务前后数据的完整性必须保持一致
    隔离性：指多个用户并发访问数据库时，一个用户的事务不能被其他用户的事务所干扰，多个并发事务之间的数据要隔离
    持久性：指一个事务一旦被提交，他对数据库中数据的改变就是永久性的，即使数据库发生故障也不应该对其有任何影响
    
    
    2、Spring事务管理高层抽象主要包括3个接口
    
    Platfrom TransactionManager事务管理器（提交回滚事务）
    
    Spring为不同的持久化框架提供了不同的Platform TransactionManager接口实现，如：
    使用Spring JDBC 或iBatis进行持久化数据时，使用DataSource TransactionManager
    使用Hibernate 3.0 版本进行持久化数据时使用Hibernate TransactionManager
    
    
    TransactionDefinition事务定义信息（隔离、传播、超时、只读）
    
    脏读：一个事务读取了另一个事务改写但未提交的数据，如果这些数据被回滚，则读到的数据时无效的
    不可重复读：在同一事务中，多次读取同一数据返回的结果有所不同
    幻读：一个事务读取了几行记录后，另一个事务插入一些记录，幻读就发生了，再后来的查询中，第一个事务就会发现有些原来
    没有的记录。
    
    事务隔离级别：
    
    DEEFAULT - 使用后端数据库默认的隔离级别（Spring选择项）
    READ_UNCOMMITED - 允许读取还未提交的修改了的数据，可能导致脏、幻、不可重复读
    READ_COMMITED - 允许并发事务已经提交后读取，可防止脏读，但幻读和不可重复读仍可发生
    REPEATABLE_READ - 对相同字段的多次读取是一致的，除非数据事务本身改变，防止脏、不可重复读，但幻读仍可发生
    SERIALIZABLE - 完全服从ACID的隔离级别，确保不发生脏、幻、不可重复读，在所有隔离级别中是最慢的，是典型的通过
    完全锁定在事务中涉及的数据表来完成的。
    
    其中Mysql默认采用REPEATABLE_READ隔离级别，ORACLE默认采用READ_COMMITED隔离级别
    
    事务传播行为：（五种）
    1、REQUIRED --支持当前事务，如果当前没有事务，就新建一个事务，这是常见的选择。
    2、SURPPORTS -- 支持当前事务，如果当前没有事务，就以非事务的方式执行。
    3、MANDATORY -- 支持当前事务，如果当前没有事务，就抛出异常
    4、REQUIRES_NEW --新建事务，如果当前存在事务，把当前事务挂起
    5、NOT_SUPPORTED --以非事务方式执行操作，如果当前存在事务，就把当前事务挂起
    6、NEVER -- 以非事务方式执行，如果当前存在事务，则抛出异常
    7、NESTED -- 如果当前存在事务，则在嵌套事务内执行，如果当前没有事务，则进行REQUIRED类似的操作，
    拥有多个可以回滚的保存点，内部回滚不会对外部事务产生影响，只对DataSourceTransactionManager有效。
    
    Transaction Status 事务具体运行状态
    
    3、Spring提供了以下方法控制事务
     编程式事务管理 利用TransactionTemplate将多个DAO操作封装起来
     
     声明式事务管理（基于SpringAOP配置控制）
       基于TransactionProxyFactoryBean方式，需要为每个进行事务管理的类，配置一个TransactionProxyFactoryBean
       进行增强。
       
       
     基于XML配置（经常使用）
        一旦配置好之后，类上不需要添加任何东西,如果Action作为目标对象切入事务，需要在<aop:config>元素里添加
        proxy-target-class="true"属性，原因是通知Spring框架采用CGLIB技术生成具有事务管理功能的Action类。
        
     基于注解(配置简单，经常使用)
     在applicationContext.xml中开启事务注解配置。(applicationContext.xml中只需定义Bean并追
                 加以下元素)
     <bean id="txManager" class="...">
       <property name="sessionFactory">
       </property>
     <tx:annotation-driven transaction-manager="txManager"/>
     在目标组件类中使用@Transactional，该标记可定义在类前或方法前。
     
     
#### 7、     Spring 事务底层原理
    Spring事务的本质其实就是数据库对事务的支持，没有数据库事务支持，spring是无法提供事务功能的。对于
    纯JDBC操作数据库，想用用到事务，可以按照以下步骤：
    1、获取连接：Connection con = DriverManager.getConnection（）
    2、开启事务：con.setAutoCommit(true/false)
    3、执行CRUD
    4、提交事务/回滚事务：con.commit() / con.rollback()
    5、关闭连接：con.close()
    
    使用Spring的事务管理功能后，可以不用再写步骤2和4的代码，而是由spring自动完成。
    以注解方式为例，配置文件开启注解驱动，在相关的类和方法上通过注解@Transactional标识，Spring在启动
    的时候会去解析生成的相关Bean，这个时候会查看拥有相关注解的类和方法，并且为这些类和方法生成代理，并根据
    @Transaction的相关参数进行相关配置注入，这样就在代理中为我们把相关的事务处理掉了（开启正常提交事务，
    异常回滚事务）。真正的数据库层的事务提交和回滚是通过binlog 和 redo log实现的。
    
    
#### 8、如何自定义注解实现功能


#### 9、Spring MVC 运行流程

    1、用户发送请求到DispatchServlet
    2、DispatchServlet根据请求路径查询具体的Handler
    3、HandlerMapping返回一个HandlerExcutionChain给DispatchServlet
    HandlerExcutionChain是Handler和Interceptor集合
    4、DispatchServlet调用HandlerAdapter适配器
    5、提取Request中的模型数据，填充Handler入参，开始执行Handler（Controller），在填充Controller的入参过程中
    根据你的配置，Spring帮助做一些额外操作。
    HttpMessageConveter：将请求消息（如Json、xml等数据）转换成一个对象，将对象转换为指定的相应信息。
    数据转换：将请求消息进行数据格式化，如将字符串转换成格式化数字或格式化日期
    数据验证：验证数据的有效性（长度、格式），验证结果存储到BingdingResult或Error中
    
    6、Handler执行完毕后，向DispatchServlet返回一个ModelAndView对象然后调用拦截器的postHandle方法。
    
    7、根据返回的ModelAndView，选择一个合适的ViewResolver（必须已经注册到Spring容器中的ViewResolver）
    返回给DispatchServlet
    
    8、ViewResolver结合Model和View，来渲染视图。
    
    9、将渲染结果返回给客户端。
    
    10、执行拦截器的afterCompletion方法
    
 #### 10、   Spring MVC 启动流程
    当一个web应用部署到一个容器中，在web应用开始响应客户端请求前，需要执行以下步骤：
    1、在web.xml中，通过为每一个时间监听者创建一个实例。
    2、通过调用contextInitialized（），实现ServletContextListener接口，实例化监听者。
    3、在web.xml中，通过为每一个过滤器创建一个实例，并调用每个过滤器实例的init（）方法。
    4、在web.xml中，通过为每一个servlet创建一个实例，并要包含元素的值，为这个servlet调用init（）方法。
    
    servlet和Filter初始化前和销毁后，都会给实现了servletContextListener接口的监听器发出相应的通知。
    
 #### 11、   Spring 的单例实现原理
    Spring采用单例注册表的特殊方式实现单例模式，这个注册表的缓存是ConcurrentHashMap
    
#### 12、    Spring 框架中用到了哪些设计模式
    1、工厂模式
    GOF23种设计模式之一，属于创建模式的一种，工厂模式可将Java对象的调用者从被调用者的实现逻辑中分离出来，
    调用者只需关心被调用者必须满足的规则（接口），而不必关心实例的具体实现过程。工厂模式由抽象产品（接口）
    具体产品（实现类）、生产者（工厂类）三种角色组成
    
    spring中的工厂模式：
    Spring在各种BeanFactory以及ApplicationContext创建中都用到了典型的工厂方法模式，BeanFactory是顶级接口，
    有三个子接口：ListableBeanFactory、HierarchicalBeanFactory和AutowireCapableBeanFactory，这三个子接口
    集成了顶级接口并对BeanFactory的功能进行了增强，称为二级接口；ConfigurableBeanFactory对二级接口
    HierarchicalBeanFactory进行了再次增强，它还继承了另一个外来的接口SingletonBeanRegistry，可以被称为
    三级接口；ConfigurableListableBeanFactory是一个更强大的接口，继承了上述的所有接口，称为四级接口。其余
    的为抽象类，实现了Spring Bean四级接口所定义的所有功能。
    
### Netty

####  1、为什么选择 Netty
     Netty是一个高性能、异步事件驱动的NIO框架。他提供了对TCP、UDP和文件传输的支持，作为一个异步NIO框架，
     Netty的所有IO操作都是异步非阻塞的，通过Future-Listener机制，用户可以方便的主动获取或者通过通知机制获得IO
     操作结果，作为当前最流行的NIO框架，Netty在互联网领域，大数据分布式计算领域、游戏、通信得到广泛应用
     
     与Netty同样的NIO框架还有MINA，Netty的主导者与Mina的主导者是同一个人，在设计理念上与Mina基本一致，
     Mina出身于开源界的大牛Apache组织，Netty出身于商业开源大亨Jboss。 
     这几年Netty社区相对比较活跃，所以我们就先选择Netty作为入手网络编程的首选

           
          

    
，
    

    



       
    
    
    
    
    
    

    
    


    

  
    
    
    

    

    
    
   

    
    
    

    
    
    
    
    
    
    
    

    
    
    

    
        
   
   
   
    


    


    


    
    

    
    

    
